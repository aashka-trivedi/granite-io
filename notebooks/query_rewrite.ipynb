{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of LoRA adapter for query rewrite\n",
    "\n",
    "This notebook shows the usage of the IO processor for the Granite query rewrite\n",
    "intrisic, also known as the [LoRA Adapter for Query Rewrite](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-query-rewrite\n",
    ")\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "models on your own server. To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants \n",
    "`openai_base_url`, `openai_base_model_name` and `openai_lora_model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "from granite_io.io.query_rewrite import QueryRewriteIOProcessor\n",
    "from granite_io.io.granite_3_3.input_processors.granite_3_3_input_processor import (\n",
    "    Granite3Point3Inputs,\n",
    ")\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from granite_io import make_backend\n",
    "from granite_io.io.rag_agent_lib import obtain_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "base_model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "# TEMPORARY: Load LoRA adapter locally\n",
    "lora_model_name = \"query_rewrite\"\n",
    "run_server = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 17:18:46 Running: /proj/dmfexp/8cc/krishna/miniforge3/envs/granite-io/bin/vllm serve ibm-granite/granite-3.3-8b-instruct --port 43151 --gpu-memory-utilization 0.45 --max-model-len 32768 --guided_decoding_backend outlines --device auto --enforce-eager --enable-lora --max_lora_rank 64 --lora-modules /proj/dmfexp/8cc/hf_home/hub/models--ibm-granite--granite-3.3-8b-rag-agent-lib/snapshots/8023ff6dfdbbc28633b15181477c6504b28c2a8e/query_rewrite_lora=/proj/dmfexp/8cc/hf_home/hub/models--ibm-granite--granite-3.3-8b-rag-agent-lib/snapshots/8023ff6dfdbbc28633b15181477c6504b28c2a8e/query_rewrite_lora\n",
      "INFO 06-19 17:18:50 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 06-19 17:18:51 api_server.py:912] vLLM API server version 0.7.3\n",
      "INFO 06-19 17:18:51 api_server.py:913] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.3-8b-instruct', config='', host=None, port=43151, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='/proj/dmfexp/8cc/hf_home/hub/models--ibm-granite--granite-3.3-8b-rag-agent-lib/snapshots/8023ff6dfdbbc28633b15181477c6504b28c2a8e/query_rewrite_lora', path='/proj/dmfexp/8cc/hf_home/hub/models--ibm-granite--granite-3.3-8b-rag-agent-lib/snapshots/8023ff6dfdbbc28633b15181477c6504b28c2a8e/query_rewrite_lora', base_model_name=None)], prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.3-8b-instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=32768, guided_decoding_backend='outlines', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.45, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x14cc967d4430>)\n",
      "INFO 06-19 17:18:51 api_server.py:209] Started engine process with PID 2143849\n",
      "INFO 06-19 17:18:55 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 06-19 17:18:56 config.py:549] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 06-19 17:18:56 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 06-19 17:18:56 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 06-19 17:19:02 config.py:549] This model supports multiple tasks: {'score', 'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 06-19 17:19:02 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 06-19 17:19:02 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 06-19 17:19:02 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='ibm-granite/granite-3.3-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.3-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ibm-granite/granite-3.3-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \n",
      "INFO 06-19 17:19:03 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 06-19 17:19:04 model_runner.py:1110] Starting to load model ibm-granite/granite-3.3-8b-instruct...\n",
      "INFO 06-19 17:19:04 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.86s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.90s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.31s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.51s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.55s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-19 17:19:11 model_runner.py:1115] Loading model weights took 15.2531 GB\n",
      "INFO 06-19 17:19:11 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 06-19 17:19:13 worker.py:267] Memory profiling takes 2.31 seconds\n",
      "INFO 06-19 17:19:13 worker.py:267] the current vLLM instance can use total_gpu_memory (79.21GiB) x gpu_memory_utilization (0.45) = 35.64GiB\n",
      "INFO 06-19 17:19:13 worker.py:267] model weights take 15.25GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 3.38GiB; the rest of the memory reserved for KV Cache is 16.85GiB.\n",
      "INFO 06-19 17:19:13 executor_base.py:111] # cuda blocks: 6902, # CPU blocks: 1638\n",
      "INFO 06-19 17:19:13 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 3.37x\n",
      "INFO 06-19 17:19:15 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 4.18 seconds\n",
      "INFO 06-19 17:19:15 serving_models.py:174] Loaded new LoRA adapter: name '/proj/dmfexp/8cc/hf_home/hub/models--ibm-granite--granite-3.3-8b-rag-agent-lib/snapshots/8023ff6dfdbbc28633b15181477c6504b28c2a8e/query_rewrite_lora', path '/proj/dmfexp/8cc/hf_home/hub/models--ibm-granite--granite-3.3-8b-rag-agent-lib/snapshots/8023ff6dfdbbc28633b15181477c6504b28c2a8e/query_rewrite_lora'\n",
      "INFO 06-19 17:19:15 api_server.py:958] Starting vLLM API server on http://0.0.0.0:43151\n",
      "INFO 06-19 17:19:15 launcher.py:23] Available routes are:\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /docs, Methods: HEAD, GET\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /health, Methods: GET\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /ping, Methods: POST, GET\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /tokenize, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /detokenize, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /v1/models, Methods: GET\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /version, Methods: GET\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /v1/completions, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /v1/embeddings, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /pooling, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /score, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /v1/score, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /rerank, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /v1/rerank, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /v2/rerank, Methods: POST\n",
      "INFO 06-19 17:19:15 launcher.py:31] Route: /invocations, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [2143533]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:39580 - \"GET /ping HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    # Download and cache the model's LoRA adapter.\n",
    "    lora_model_path = obtain_lora(lora_model_name)\n",
    "    print(f\"Local path to LoRA adapter: {lora_model_path}\")\n",
    "    server = LocalVLLMServer(\n",
    "        base_model_name, lora_adapters=[(lora_model_name, lora_model_path)]\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    lora_backend = server.make_lora_backend(lora_model_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # Modify the constants here as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    openai_base_model_name = base_model_name\n",
    "    openai_lora_model_name = lora_model_name\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_base_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_lora_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Granite3Point3Inputs(messages=[AssistantMessage(content='Welcome to pet questions!', role='assistant', tool_calls=[], reasoning_content=None, citations=None, documents=None, hallucinations=None, stop_reason=None), UserMessage(content='I have two pets, a dog named Rex and a cat named Lucy.', role='user'), AssistantMessage(content='Great, what would you like to share about them?', role='assistant', tool_calls=[], reasoning_content=None, citations=None, documents=None, hallucinations=None, stop_reason=None), UserMessage(content='Rex spends a lot of time in the backyard and outdoors, and Luna is always inside.', role='user'), AssistantMessage(content='Sounds good! Rex must love exploring outside, while Lucy probably enjoys her cozy indoor life.', role='assistant', tool_calls=[], reasoning_content=None, citations=None, documents=None, hallucinations=None, stop_reason=None), UserMessage(content='But is he more likely to get fleas because of that?', role='user')], tools=[], generate_inputs=GenerateInputs(prompt=None, model=None, best_of=None, echo=None, frequency_penalty=None, logit_bias=None, logprobs=None, max_tokens=None, n=None, presence_penalty=None, stop=None, stream=None, stream_options=None, suffix=None, temperature=0.0, top_p=None, user=None, extra_headers=None, extra_body={}), documents=[], controls=None, thinking=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an example chat completion with a short conversation.\n",
    "chat_input = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"assistant\", \"content\": \"Welcome to pet questions!\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I have two pets, a dog named Rex and a cat named Lucy.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Great, what would you like to share about them?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Rex spends a lot of time in the backyard and outdoors, \"\n",
    "                \"and Luna is always inside.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Sounds good! Rex must love exploring outside, while Lucy \"\n",
    "                \"probably enjoys her cozy indoor life.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"But is he more likely to get fleas because of that?\",\n",
    "            },\n",
    "        ],\n",
    "        \"generate_inputs\": {\"temperature\": 0.0},\n",
    "    }\n",
    ")\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-19 17:19:16 logger.py:39] Received request cmpl-c7095b33458d4ecb9d52c6ea8a334c14-0: prompt: '<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday\\'s Date: June 19, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Welcome to pet questions!<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I have two pets, a dog named Rex and a cat named Lucy.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Great, what would you like to share about them?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>Rex spends a lot of time in the backyard and outdoors, and Luna is always inside.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Sounds good! Rex must love exploring outside, while Lucy probably enjoys her cozy indoor life.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>But is he more likely to get fleas because of that?<|end_of_text|>\\n<|start_of_role|>query_to_rewrite<|end_of_role|>But is he more likely to get fleas because of that?<|end_of_text|>\\n<|start_of_role|>rewrite: Given the conversation history above and the specific query provided in the \\'query_to_rewrite\\' role, rewrite that query into a standalone question that captures the user\\'s intent without requiring the conversation context. If the query is already clear and standalone, output it as is.\\nYour output should be a JSON structure with the rewritten question:\\n```json\\n{\\n    \"rewritten_question\": \"YOUR_REWRITTEN_QUESTION_HERE\"\\n}\\n```<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex='```json\\\\n\\\\{\\\\s*\\\\\"rewritten_question\\\\\"\\\\s*:\\\\s*\\\\\"[^\\\\\"]*\\\\\"\\\\s*\\\\}\\\\n```', choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 35, 43, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 2448, 884, 312, 17247, 19551, 47330, 32, 0, 203, 49152, 17594, 49153, 19016, 372, 25792, 10017, 19, 0, 203, 49152, 496, 49153, 59, 1159, 3134, 298, 5231, 30, 312, 27435, 8189, 559, 327, 461, 312, 10501, 8189, 32423, 107, 32, 0, 203, 49152, 17594, 49153, 45236, 30, 2769, 2268, 844, 2124, 372, 8623, 2625, 3026, 49, 0, 203, 49152, 496, 49153, 68, 327, 1869, 1494, 312, 8215, 432, 1133, 328, 322, 1751, 40937, 461, 963, 1409, 1734, 30, 461, 498, 12120, 438, 5182, 6517, 32, 0, 203, 49152, 17594, 49153, 40846, 4644, 19, 559, 327, 2298, 14290, 27514, 299, 11127, 30, 2218, 32423, 107, 7801, 931, 3597, 1269, 7791, 2098, 6637, 328, 18801, 12535, 32, 0, 203, 49152, 496, 49153, 11275, 438, 938, 1829, 11353, 372, 622, 296, 274, 302, 3301, 432, 688, 49, 0, 203, 49152, 1217, 81, 452, 81, 22179, 49153, 11275, 438, 938, 1829, 11353, 372, 622, 296, 274, 302, 3301, 432, 688, 49, 0, 203, 49152, 22179, 44, 15273, 322, 19509, 8142, 3684, 461, 322, 2818, 2467, 3943, 328, 322, 330, 1217, 81, 452, 81, 22179, 25, 4203, 30, 21817, 688, 2467, 1991, 312, 27933, 7000, 688, 40341, 322, 1256, 1182, 8927, 2876, 33783, 322, 19509, 1619, 32, 1670, 322, 2467, 438, 3425, 4233, 461, 27933, 30, 1688, 561, 619, 438, 32, 203, 10195, 1688, 1395, 526, 312, 3398, 5193, 623, 322, 316, 15557, 7000, 44, 203, 914, 1260, 203, 109, 284, 313, 268, 15557, 81, 4594, 563, 313, 25735, 81, 613, 2591, 780, 32986, 81, 36831, 1016, 81, 4386, 20, 203, 111, 203, 914, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-19 17:19:22 engine.py:280] Added request cmpl-c7095b33458d4ecb9d52c6ea8a334c14-0.\n",
      "INFO 06-19 17:19:22 metrics.py:455] Avg prompt throughput: 41.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO:     127.0.0.1:39582 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"Since Rex spends a lot of time outdoors, is he more prone to getting fleas compared to Lucy who stays inside?\",\n",
      "  \"role\": \"user\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the I/O processor for the LoRA adapter\n",
    "io_proc = QueryRewriteIOProcessor(backend)\n",
    "\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(chat_input)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU resources\n",
    "if \"server\" in locals():\n",
    "    server.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
