{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cadedd9-bc8b-4986-b16a-4e3540ff2e2b",
   "metadata": {},
   "source": [
    "# Demonstration of the Granite RAG Context Relevance Intrisic\n",
    "\n",
    "This notebook shows the usage of the IO processor for the Granite RAG context relevance intrisic, \n",
    "also known as the [LoRA Adapter for Context Relevance Classifier]()\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "models on your own server. To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants \n",
    "`openai_base_url`, `openai_base_model_name` and `openai_lora_model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f3624f-56ec-4a8f-84be-fb499ac77c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# if your notebookâ€™s working dir is the project root:\n",
    "src_path = os.path.abspath(\"../src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a6384b-d441-4dc6-821c-0ee928046fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.granite_3_3.input_processors.granite_3_3_input_processor import (\n",
    "    Granite3Point3Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from IPython.display import display, Markdown\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from granite_io.io.context_relevancy import ContextRelevancyIOProcessor, ContextRelevancyCompositeIOProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "926261b4-74f8-40f3-bcdd-da21fcb9e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "base_model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "# TEMPORARY: Load LoRA adapter locally\n",
    "lora_model_name = \"local-granite-3.3-8b-lora-rag-context-relevancy\"\n",
    "run_server = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b868bf-15e8-4b37-8f8f-d16e40dd0f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 20:46:45 Running: /proj/dmfexp/8cc/krishna/miniforge3/envs/granite-io/bin/vllm serve ibm-granite/granite-3.3-8b-instruct --port 56741 --gpu-memory-utilization 0.45 --max-model-len 32768 --guided_decoding_backend outlines --device auto --enforce-eager --enable-lora --max_lora_rank 64 --lora-modules local-granite-3.3-8b-lora-rag-context-relevancy=local-granite-3.3-8b-lora-rag-context-relevancy\n",
      "INFO 06-13 20:46:49 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 06-13 20:46:50 api_server.py:912] vLLM API server version 0.7.3\n",
      "INFO 06-13 20:46:50 api_server.py:913] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.3-8b-instruct', config='', host=None, port=56741, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='local-granite-3.3-8b-lora-rag-context-relevancy', path='local-granite-3.3-8b-lora-rag-context-relevancy', base_model_name=None)], prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.3-8b-instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=32768, guided_decoding_backend='outlines', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.45, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x14daed207d00>)\n",
      "INFO 06-13 20:46:50 api_server.py:209] Started engine process with PID 3029936\n",
      "INFO 06-13 20:46:54 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 06-13 20:46:56 config.py:549] This model supports multiple tasks: {'reward', 'embed', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 06-13 20:46:56 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 06-13 20:46:56 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 06-13 20:47:00 config.py:549] This model supports multiple tasks: {'generate', 'classify', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 06-13 20:47:00 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 06-13 20:47:00 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 06-13 20:47:00 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='ibm-granite/granite-3.3-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.3-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ibm-granite/granite-3.3-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \n",
      "INFO 06-13 20:47:01 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 06-13 20:47:02 model_runner.py:1110] Starting to load model ibm-granite/granite-3.3-8b-instruct...\n",
      "INFO 06-13 20:47:02 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.92s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.96s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.36s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.60s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:47:09 model_runner.py:1115] Loading model weights took 15.2531 GB\n",
      "INFO 06-13 20:47:09 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 06-13 20:47:12 worker.py:267] Memory profiling takes 2.32 seconds\n",
      "INFO 06-13 20:47:12 worker.py:267] the current vLLM instance can use total_gpu_memory (79.21GiB) x gpu_memory_utilization (0.45) = 35.64GiB\n",
      "INFO 06-13 20:47:12 worker.py:267] model weights take 15.25GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 3.38GiB; the rest of the memory reserved for KV Cache is 16.85GiB.\n",
      "INFO 06-13 20:47:12 executor_base.py:111] # cuda blocks: 6902, # CPU blocks: 1638\n",
      "INFO 06-13 20:47:12 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 3.37x\n",
      "INFO 06-13 20:47:13 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 4.16 seconds\n",
      "INFO 06-13 20:47:14 serving_models.py:174] Loaded new LoRA adapter: name 'local-granite-3.3-8b-lora-rag-context-relevancy', path 'local-granite-3.3-8b-lora-rag-context-relevancy'\n",
      "INFO 06-13 20:47:14 api_server.py:958] Starting vLLM API server on http://0.0.0.0:56741\n",
      "INFO 06-13 20:47:14 launcher.py:23] Available routes are:\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /docs, Methods: HEAD, GET\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /health, Methods: GET\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /ping, Methods: GET, POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /tokenize, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /detokenize, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /v1/models, Methods: GET\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /version, Methods: GET\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /v1/completions, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /v1/embeddings, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /pooling, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /score, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /v1/score, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /rerank, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /v1/rerank, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /v2/rerank, Methods: POST\n",
      "INFO 06-13 20:47:14 launcher.py:31] Route: /invocations, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3029641]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:60046 - \"GET /ping HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(\n",
    "        base_model_name, lora_adapters=[(lora_model_name, lora_model_name)]\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    lora_backend = server.make_lora_backend(lora_model_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # Modify the constants here as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    openai_base_model_name = base_model_name\n",
    "    openai_lora_model_name = lora_model_name\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_base_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_lora_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b186b25-140c-4461-85cf-d5429690c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a short conversation.\n",
    "# Base conversation about pets\n",
    "base_messages = [\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm here to help you prepare for your job interview!\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I have a job interview next week for a marketing manager position.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Congratulations! Marketing manager is an exciting role. How are you feeling about it?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm nervous because I haven't interviewed in years, and this is a big career move for me.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"It's natural to feel nervous, but preparation will help boost your confidence.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What should I expect them to ask about my experience with social media campaigns as a marketing manager?\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635826d2-8b5c-45d2-a243-af5717710ed7",
   "metadata": {},
   "source": [
    "## Relevant Document Context Relevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037208d4-be9d-4269-ae27-16ba96c6132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: RELEVANT document - directly addresses outdoor pets and flea risk\n",
    "chat_input_relevant = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": base_messages,\n",
    "        \"documents\": [{\"text\":\n",
    "        \"Marketing manager interviews often focus on campaign experience and measurable results. \"\n",
    "        \"Expect questions about social media ROI, audience engagement metrics, and conversion rates. \"\n",
    "        \"Prepare specific examples of campaigns you've managed, including budget, timeline, and outcomes. \"\n",
    "        \"Interviewers may ask about your experience with different social media platforms and their unique audiences. \"\n",
    "        \"Be ready to discuss how you measure campaign success and adjust strategies based on performance data. \"\n",
    "        \"Knowledge of current social media trends and emerging platforms demonstrates industry awareness.\"}\n",
    "        ],\n",
    "        \"generate_inputs\": {\"temperature\": 0.0},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0459fdf7-1faf-4ed7-91a8-0222d2a73662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:47:15 logger.py:39] Received request cmpl-5df174b3e62e43c9a9fe97710732f81c-0: prompt: '<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday\\'s Date: June 13, 2025.\\nYou are Granite, developed by IBM. Write the response to the user\\'s input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.<|end_of_text|>\\n<|start_of_role|>document {\"document_id\": \"1\"}<|end_of_role|>\\nMarketing manager interviews often focus on campaign experience and measurable results. Expect questions about social media ROI, audience engagement metrics, and conversion rates. Prepare specific examples of campaigns you\\'ve managed, including budget, timeline, and outcomes. Interviewers may ask about your experience with different social media platforms and their unique audiences. Be ready to discuss how you measure campaign success and adjust strategies based on performance data. Knowledge of current social media trends and emerging platforms demonstrates industry awareness.<|end_of_text|><|start_of_role|>assistant<|end_of_role|>I\\'m here to help you prepare for your job interview!<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I have a job interview next week for a marketing manager position.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Congratulations! Marketing manager is an exciting role. How are you feeling about it?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I\\'m nervous because I haven\\'t interviewed in years, and this is a big career move for me.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>It\\'s natural to feel nervous, but preparation will help boost your confidence.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>final_user_query<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>context_relevance: Analyze the provided document in relation to the final user query from the conversation. Determine if the document contains information that could help answer the final user query. Output \\'relevant\\' if the document contains substantial information directly useful for answering the final user query. Output \\'partially relevant\\' if the document contains some related information that could partially help answer the query, or if you are uncertain about the relevance - err on the side of \\'partially relevant\\' when in doubt. Output \\'irrelevant\\' only if the document clearly contains no information that could help answer the final user query. When uncertain, choose \\'partially relevant\\' rather than \\'irrelevant\\'. Your output should be a JSON structure with the context relevance classification:\\n```json\\n{\\n    \"context_relevance\": \"YOUR_CONTEXT_RELEVANCE_CLASSIFICATION_HERE\"\\n}\\n```<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex='```json\\\\n\\\\{\\\\s*\\\\\"context_relevance\\\\\"\\\\s*:\\\\s*\\\\\"(irrelevant|relevant|partially relevant)\\\\\"\\\\s*\\\\}\\\\n```', choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 35, 37, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 5950, 322, 1789, 372, 322, 1256, 1182, 1509, 810, 33220, 3014, 299, 623, 322, 44052, 328, 322, 3943, 12827, 32, 1670, 322, 2471, 5349, 372, 7592, 322, 7000, 438, 646, 3304, 328, 322, 12827, 30, 7956, 322, 1256, 688, 322, 7000, 4881, 526, 40031, 4122, 544, 322, 3304, 706, 32, 0, 203, 49152, 2812, 3447, 2812, 81, 314, 563, 313, 35, 3612, 49153, 203, 4494, 8842, 7884, 1426, 4338, 12270, 9280, 544, 24405, 12409, 461, 15499, 305, 444, 3276, 32, 9177, 10017, 2625, 15332, 7444, 11253, 59, 30, 36021, 14707, 3877, 8607, 30, 461, 11741, 26059, 32, 20920, 2818, 8272, 432, 24405, 101, 844, 4763, 12852, 30, 6237, 23912, 30, 24253, 30, 461, 46197, 32, 4803, 1071, 483, 1631, 7660, 2625, 1370, 12409, 623, 3449, 15332, 7444, 17503, 461, 3623, 5799, 21091, 91, 3015, 32, 4261, 8753, 372, 23273, 2624, 844, 9413, 24405, 3117, 461, 10891, 35678, 4122, 544, 7871, 706, 32, 38401, 432, 1550, 15332, 7444, 31010, 101, 461, 2642, 265, 2563, 17503, 35968, 29404, 18934, 4321, 32, 0, 49152, 17594, 49153, 59, 3464, 2442, 372, 3049, 844, 11656, 436, 1370, 3992, 1426, 1071, 19, 0, 203, 49152, 496, 49153, 59, 1159, 312, 3992, 1426, 1071, 2354, 8209, 436, 312, 43004, 7884, 2747, 32, 0, 203, 49152, 17594, 49153, 609, 44688, 17480, 19, 7778, 8842, 7884, 438, 600, 13489, 37869, 4203, 32, 4971, 884, 844, 42289, 2625, 561, 49, 0, 203, 49152, 496, 49153, 59, 3464, 47428, 36860, 3301, 439, 14562, 1330, 1426, 1071, 318, 328, 11274, 30, 461, 458, 438, 312, 6524, 45378, 5169, 436, 597, 32, 0, 203, 49152, 17594, 49153, 1011, 1182, 19844, 372, 10871, 47428, 36860, 30, 1273, 42357, 1098, 3049, 9760, 1370, 24462, 32, 0, 203, 49152, 496, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 3220, 81, 496, 81, 1217, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 1819, 81, 268, 40289, 44, 2244, 12884, 322, 3943, 1825, 328, 14772, 372, 322, 1158, 1256, 2467, 645, 322, 19509, 32, 22022, 415, 322, 1825, 4304, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5582, 330, 30762, 25, 415, 322, 1825, 4304, 14226, 2471, 7029, 6364, 436, 7592, 299, 322, 1158, 1256, 2467, 32, 5582, 330, 2050, 6321, 10088, 25, 415, 322, 1825, 4304, 1629, 5886, 2471, 688, 3065, 34659, 3049, 7592, 322, 2467, 30, 556, 415, 844, 884, 31909, 504, 2625, 322, 316, 40289, 429, 1137, 544, 322, 5209, 432, 330, 2050, 6321, 10088, 25, 1412, 328, 40062, 32, 5582, 330, 476, 30762, 25, 1755, 415, 322, 1825, 27298, 4304, 1289, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5076, 31909, 504, 30, 9173, 330, 2050, 6321, 10088, 25, 9283, 2784, 330, 476, 30762, 2473, 10604, 1688, 1395, 526, 312, 3398, 5193, 623, 322, 1619, 316, 40289, 17717, 44, 203, 914, 1260, 203, 109, 284, 313, 1819, 81, 268, 40289, 563, 313, 25735, 81, 13952, 81, 613, 815, 72, 6307, 81, 25418, 38075, 9590, 81, 4386, 20, 203, 111, 203, 914, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-13 20:47:20 engine.py:280] Added request cmpl-5df174b3e62e43c9a9fe97710732f81c-0.\n",
      "INFO 06-13 20:47:21 metrics.py:455] Avg prompt throughput: 80.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n",
      "INFO:     127.0.0.1:60052 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"relevant\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"tool_calls\": [],\n",
      "  \"reasoning_content\": null,\n",
      "  \"citations\": null,\n",
      "  \"documents\": null,\n",
      "  \"hallucinations\": null,\n",
      "  \"stop_reason\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "io_proc = ContextRelevancyIOProcessor(backend)\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(chat_input_relevant)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e516d73-e191-4611-8ee9-5d2bfad40e97",
   "metadata": {},
   "source": [
    "## Partially Relevant Context Relevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7399d937-9a82-411c-968d-52b4ae6edd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input_partially_relevant = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": base_messages,\n",
    "        \"documents\": [{\"text\":\n",
    "        \"Job interviews typically follow a structured format with behavioral and technical questions. \"\n",
    "        \"Preparing specific examples using the STAR method helps answer behavioral questions effectively. \"\n",
    "        \"Research the company's mission, values, and recent news before your interview. \"\n",
    "        \"Dress appropriately for the company culture and arrive 10-15 minutes early. \"\n",
    "        \"Prepare thoughtful questions to ask the interviewer about the role and company. \"\n",
    "        \"Following up with a thank-you email within 24 hours shows professionalism and interest.\"}\n",
    "        ],\n",
    "        \"generate_inputs\": {\"temperature\": 0.0},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62685c9f-2205-4918-85b9-0fe53570c789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:47:21 logger.py:39] Received request cmpl-4a90ef40311f4f3abb8e40039cee463b-0: prompt: '<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday\\'s Date: June 13, 2025.\\nYou are Granite, developed by IBM. Write the response to the user\\'s input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.<|end_of_text|>\\n<|start_of_role|>document {\"document_id\": \"1\"}<|end_of_role|>\\nJob interviews typically follow a structured format with behavioral and technical questions. Preparing specific examples using the STAR method helps answer behavioral questions effectively. Research the company\\'s mission, values, and recent news before your interview. Dress appropriately for the company culture and arrive 10-15 minutes early. Prepare thoughtful questions to ask the interviewer about the role and company. Following up with a thank-you email within 24 hours shows professionalism and interest.<|end_of_text|><|start_of_role|>assistant<|end_of_role|>I\\'m here to help you prepare for your job interview!<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I have a job interview next week for a marketing manager position.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Congratulations! Marketing manager is an exciting role. How are you feeling about it?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I\\'m nervous because I haven\\'t interviewed in years, and this is a big career move for me.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>It\\'s natural to feel nervous, but preparation will help boost your confidence.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>final_user_query<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>context_relevance: Analyze the provided document in relation to the final user query from the conversation. Determine if the document contains information that could help answer the final user query. Output \\'relevant\\' if the document contains substantial information directly useful for answering the final user query. Output \\'partially relevant\\' if the document contains some related information that could partially help answer the query, or if you are uncertain about the relevance - err on the side of \\'partially relevant\\' when in doubt. Output \\'irrelevant\\' only if the document clearly contains no information that could help answer the final user query. When uncertain, choose \\'partially relevant\\' rather than \\'irrelevant\\'. Your output should be a JSON structure with the context relevance classification:\\n```json\\n{\\n    \"context_relevance\": \"YOUR_CONTEXT_RELEVANCE_CLASSIFICATION_HERE\"\\n}\\n```<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex='```json\\\\n\\\\{\\\\s*\\\\\"context_relevance\\\\\"\\\\s*:\\\\s*\\\\\"(irrelevant|relevant|partially relevant)\\\\\"\\\\s*\\\\}\\\\n```', choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 35, 37, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 5950, 322, 1789, 372, 322, 1256, 1182, 1509, 810, 33220, 3014, 299, 623, 322, 44052, 328, 322, 3943, 12827, 32, 1670, 322, 2471, 5349, 372, 7592, 322, 7000, 438, 646, 3304, 328, 322, 12827, 30, 7956, 322, 1256, 688, 322, 7000, 4881, 526, 40031, 4122, 544, 322, 3304, 706, 32, 0, 203, 49152, 2812, 3447, 2812, 81, 314, 563, 313, 35, 3612, 49153, 203, 3782, 1426, 4338, 18214, 1976, 312, 28546, 2179, 623, 5853, 279, 461, 22131, 10017, 32, 7474, 26124, 2818, 8272, 1471, 322, 3849, 783, 1411, 15761, 7592, 5853, 279, 10017, 29419, 32, 18699, 322, 10512, 1182, 28860, 30, 2078, 30, 461, 8289, 14165, 2670, 1370, 1426, 1071, 32, 475, 714, 36808, 436, 322, 10512, 27668, 461, 2099, 5864, 225, 35, 34, 31, 35, 39, 10135, 15883, 32, 20920, 10889, 2790, 10017, 372, 7660, 322, 1426, 11429, 2625, 322, 4203, 461, 10512, 32, 41339, 973, 623, 312, 17715, 31, 9619, 4096, 4797, 225, 36, 38, 11632, 9830, 39073, 5738, 461, 10219, 32, 0, 49152, 17594, 49153, 59, 3464, 2442, 372, 3049, 844, 11656, 436, 1370, 3992, 1426, 1071, 19, 0, 203, 49152, 496, 49153, 59, 1159, 312, 3992, 1426, 1071, 2354, 8209, 436, 312, 43004, 7884, 2747, 32, 0, 203, 49152, 17594, 49153, 609, 44688, 17480, 19, 7778, 8842, 7884, 438, 600, 13489, 37869, 4203, 32, 4971, 884, 844, 42289, 2625, 561, 49, 0, 203, 49152, 496, 49153, 59, 3464, 47428, 36860, 3301, 439, 14562, 1330, 1426, 1071, 318, 328, 11274, 30, 461, 458, 438, 312, 6524, 45378, 5169, 436, 597, 32, 0, 203, 49152, 17594, 49153, 1011, 1182, 19844, 372, 10871, 47428, 36860, 30, 1273, 42357, 1098, 3049, 9760, 1370, 24462, 32, 0, 203, 49152, 496, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 3220, 81, 496, 81, 1217, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 1819, 81, 268, 40289, 44, 2244, 12884, 322, 3943, 1825, 328, 14772, 372, 322, 1158, 1256, 2467, 645, 322, 19509, 32, 22022, 415, 322, 1825, 4304, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5582, 330, 30762, 25, 415, 322, 1825, 4304, 14226, 2471, 7029, 6364, 436, 7592, 299, 322, 1158, 1256, 2467, 32, 5582, 330, 2050, 6321, 10088, 25, 415, 322, 1825, 4304, 1629, 5886, 2471, 688, 3065, 34659, 3049, 7592, 322, 2467, 30, 556, 415, 844, 884, 31909, 504, 2625, 322, 316, 40289, 429, 1137, 544, 322, 5209, 432, 330, 2050, 6321, 10088, 25, 1412, 328, 40062, 32, 5582, 330, 476, 30762, 25, 1755, 415, 322, 1825, 27298, 4304, 1289, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5076, 31909, 504, 30, 9173, 330, 2050, 6321, 10088, 25, 9283, 2784, 330, 476, 30762, 2473, 10604, 1688, 1395, 526, 312, 3398, 5193, 623, 322, 1619, 316, 40289, 17717, 44, 203, 914, 1260, 203, 109, 284, 313, 1819, 81, 268, 40289, 563, 313, 25735, 81, 13952, 81, 613, 815, 72, 6307, 81, 25418, 38075, 9590, 81, 4386, 20, 203, 111, 203, 914, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-13 20:47:21 engine.py:280] Added request cmpl-4a90ef40311f4f3abb8e40039cee463b-0.\n",
      "INFO:     127.0.0.1:60052 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"partially relevant\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"tool_calls\": [],\n",
      "  \"reasoning_content\": null,\n",
      "  \"citations\": null,\n",
      "  \"documents\": null,\n",
      "  \"hallucinations\": null,\n",
      "  \"stop_reason\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "io_proc = ContextRelevancyIOProcessor(backend)\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(chat_input_partially_relevant)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409286af-97d8-4e5e-aaae-c119467bb581",
   "metadata": {},
   "source": [
    "## Irrelevant Context Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d050eff-8f7b-4fe3-b77c-3564bce5c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input_irrelevant = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": base_messages,\n",
    "        \"documents\": [{\"text\":\n",
    "        \"Proper knife skills are fundamental to efficient cooking and food safety in the kitchen. \"\n",
    "        \"Different cuts like julienne, brunoise, and chiffonade serve specific culinary purposes. \"\n",
    "        \"Sharp knives are actually safer than dull ones because they require less pressure to cut. \"\n",
    "        \"Learning to properly hold and control a chef's knife takes practice and patience. \"\n",
    "        \"Professional chefs can prep vegetables much faster due to their refined knife techniques. \"\n",
    "        \"Regular knife maintenance including sharpening and proper storage extends blade life.\"\n",
    "                }\n",
    "        ],\n",
    "        \"generate_inputs\": {\"temperature\": 0.0},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d94d73-feeb-452b-ba59-cbc6c7714bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 20:47:22 logger.py:39] Received request cmpl-e80c29358df848bd8c1ea296faa785db-0: prompt: '<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday\\'s Date: June 13, 2025.\\nYou are Granite, developed by IBM. Write the response to the user\\'s input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.<|end_of_text|>\\n<|start_of_role|>document {\"document_id\": \"1\"}<|end_of_role|>\\nProper knife skills are fundamental to efficient cooking and food safety in the kitchen. Different cuts like julienne, brunoise, and chiffonade serve specific culinary purposes. Sharp knives are actually safer than dull ones because they require less pressure to cut. Learning to properly hold and control a chef\\'s knife takes practice and patience. Professional chefs can prep vegetables much faster due to their refined knife techniques. Regular knife maintenance including sharpening and proper storage extends blade life.<|end_of_text|><|start_of_role|>assistant<|end_of_role|>I\\'m here to help you prepare for your job interview!<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I have a job interview next week for a marketing manager position.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Congratulations! Marketing manager is an exciting role. How are you feeling about it?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I\\'m nervous because I haven\\'t interviewed in years, and this is a big career move for me.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>It\\'s natural to feel nervous, but preparation will help boost your confidence.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>final_user_query<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>context_relevance: Analyze the provided document in relation to the final user query from the conversation. Determine if the document contains information that could help answer the final user query. Output \\'relevant\\' if the document contains substantial information directly useful for answering the final user query. Output \\'partially relevant\\' if the document contains some related information that could partially help answer the query, or if you are uncertain about the relevance - err on the side of \\'partially relevant\\' when in doubt. Output \\'irrelevant\\' only if the document clearly contains no information that could help answer the final user query. When uncertain, choose \\'partially relevant\\' rather than \\'irrelevant\\'. Your output should be a JSON structure with the context relevance classification:\\n```json\\n{\\n    \"context_relevance\": \"YOUR_CONTEXT_RELEVANCE_CLASSIFICATION_HERE\"\\n}\\n```<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex='```json\\\\n\\\\{\\\\s*\\\\\"context_relevance\\\\\"\\\\s*:\\\\s*\\\\\"(irrelevant|relevant|partially relevant)\\\\\"\\\\s*\\\\}\\\\n```', choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 35, 37, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 5950, 322, 1789, 372, 322, 1256, 1182, 1509, 810, 33220, 3014, 299, 623, 322, 44052, 328, 322, 3943, 12827, 32, 1670, 322, 2471, 5349, 372, 7592, 322, 7000, 438, 646, 3304, 328, 322, 12827, 30, 7956, 322, 1256, 688, 322, 7000, 4881, 526, 40031, 4122, 544, 322, 3304, 706, 32, 0, 203, 49152, 2812, 3447, 2812, 81, 314, 563, 313, 35, 3612, 49153, 203, 590, 462, 22851, 6354, 22728, 884, 40250, 372, 17457, 23682, 299, 461, 17902, 26160, 328, 322, 831, 30447, 32, 43044, 11909, 101, 2124, 594, 338, 8605, 941, 30, 323, 1945, 97, 1125, 30, 461, 663, 2937, 267, 1525, 13942, 2818, 31334, 3172, 14998, 32, 44937, 22851, 4119, 884, 6723, 4257, 1012, 2784, 343, 514, 11668, 3301, 2953, 1205, 6679, 23319, 372, 11909, 32, 14292, 372, 10273, 7629, 461, 3498, 312, 44051, 1182, 22851, 6354, 8727, 18572, 461, 8402, 5467, 32, 45530, 8277, 2548, 883, 35535, 5292, 371, 6975, 5610, 15162, 7254, 372, 3623, 2488, 1441, 22851, 6354, 26114, 32, 33196, 22851, 6354, 24514, 6237, 787, 282, 1127, 299, 461, 7240, 4759, 2026, 3145, 1525, 12535, 32, 0, 49152, 17594, 49153, 59, 3464, 2442, 372, 3049, 844, 11656, 436, 1370, 3992, 1426, 1071, 19, 0, 203, 49152, 496, 49153, 59, 1159, 312, 3992, 1426, 1071, 2354, 8209, 436, 312, 43004, 7884, 2747, 32, 0, 203, 49152, 17594, 49153, 609, 44688, 17480, 19, 7778, 8842, 7884, 438, 600, 13489, 37869, 4203, 32, 4971, 884, 844, 42289, 2625, 561, 49, 0, 203, 49152, 496, 49153, 59, 3464, 47428, 36860, 3301, 439, 14562, 1330, 1426, 1071, 318, 328, 11274, 30, 461, 458, 438, 312, 6524, 45378, 5169, 436, 597, 32, 0, 203, 49152, 17594, 49153, 1011, 1182, 19844, 372, 10871, 47428, 36860, 30, 1273, 42357, 1098, 3049, 9760, 1370, 24462, 32, 0, 203, 49152, 496, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 3220, 81, 496, 81, 1217, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 1819, 81, 268, 40289, 44, 2244, 12884, 322, 3943, 1825, 328, 14772, 372, 322, 1158, 1256, 2467, 645, 322, 19509, 32, 22022, 415, 322, 1825, 4304, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5582, 330, 30762, 25, 415, 322, 1825, 4304, 14226, 2471, 7029, 6364, 436, 7592, 299, 322, 1158, 1256, 2467, 32, 5582, 330, 2050, 6321, 10088, 25, 415, 322, 1825, 4304, 1629, 5886, 2471, 688, 3065, 34659, 3049, 7592, 322, 2467, 30, 556, 415, 844, 884, 31909, 504, 2625, 322, 316, 40289, 429, 1137, 544, 322, 5209, 432, 330, 2050, 6321, 10088, 25, 1412, 328, 40062, 32, 5582, 330, 476, 30762, 25, 1755, 415, 322, 1825, 27298, 4304, 1289, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5076, 31909, 504, 30, 9173, 330, 2050, 6321, 10088, 25, 9283, 2784, 330, 476, 30762, 2473, 10604, 1688, 1395, 526, 312, 3398, 5193, 623, 322, 1619, 316, 40289, 17717, 44, 203, 914, 1260, 203, 109, 284, 313, 1819, 81, 268, 40289, 563, 313, 25735, 81, 13952, 81, 613, 815, 72, 6307, 81, 25418, 38075, 9590, 81, 4386, 20, 203, 111, 203, 914, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-13 20:47:22 engine.py:280] Added request cmpl-e80c29358df848bd8c1ea296faa785db-0.\n",
      "INFO:     127.0.0.1:60052 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"irrelevant\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"tool_calls\": [],\n",
      "  \"reasoning_content\": null,\n",
      "  \"citations\": null,\n",
      "  \"documents\": null,\n",
      "  \"hallucinations\": null,\n",
      "  \"stop_reason\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "io_proc = ContextRelevancyIOProcessor(backend)\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(chat_input_irrelevant)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
