{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cadedd9-bc8b-4986-b16a-4e3540ff2e2b",
   "metadata": {},
   "source": [
    "# Demonstration of the Granite RAG Context Relevance Intrisic\n",
    "\n",
    "This notebook shows the usage of the IO processor for the Granite RAG context relevance intrisic, \n",
    "also known as the [LoRA Adapter for Context Relevance Classifier]()\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "models on your own server. To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants \n",
    "`openai_base_url`, `openai_base_model_name` and `openai_lora_model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f3624f-56ec-4a8f-84be-fb499ac77c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# if your notebookâ€™s working dir is the project root:\n",
    "src_path = os.path.abspath(\"../src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a6384b-d441-4dc6-821c-0ee928046fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.granite_3_3.input_processors.granite_3_3_input_processor import (\n",
    "    Granite3Point3Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from IPython.display import display, Markdown\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from granite_io.io.context_relevancy import ContextRelevancyIOProcessor, ContextRelevancyCompositeIOProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "926261b4-74f8-40f3-bcdd-da21fcb9e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "base_model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "# TEMPORARY: Load LoRA adapter locally\n",
    "lora_model_name = \"local-granite-3.3-8b-lora-rag-context-relevancy\"\n",
    "run_server = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b868bf-15e8-4b37-8f8f-d16e40dd0f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04:15:12 Running: /proj/dmfexp/8cc/krishna/miniforge3/envs/granite-io/bin/vllm serve ibm-granite/granite-3.3-8b-instruct --port 45109 --gpu-memory-utilization 0.45 --max-model-len 32768 --guided_decoding_backend outlines --device auto --enforce-eager --enable-lora --max_lora_rank 64 --lora-modules local-granite-3.3-8b-lora-rag-context-relevancy=local-granite-3.3-8b-lora-rag-context-relevancy\n",
      "INFO 06-14 04:15:16 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 06-14 04:15:16 api_server.py:912] vLLM API server version 0.7.3\n",
      "INFO 06-14 04:15:16 api_server.py:913] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.3-8b-instruct', config='', host=None, port=45109, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='local-granite-3.3-8b-lora-rag-context-relevancy', path='local-granite-3.3-8b-lora-rag-context-relevancy', base_model_name=None)], prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.3-8b-instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=32768, guided_decoding_backend='outlines', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.45, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x1475b46ebd00>)\n",
      "INFO 06-14 04:15:16 api_server.py:209] Started engine process with PID 3986817\n",
      "INFO 06-14 04:15:21 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 06-14 04:15:22 config.py:549] This model supports multiple tasks: {'generate', 'classify', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 06-14 04:15:22 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 06-14 04:15:22 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 06-14 04:15:27 config.py:549] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 06-14 04:15:27 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 06-14 04:15:27 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 06-14 04:15:27 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='ibm-granite/granite-3.3-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.3-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ibm-granite/granite-3.3-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \n",
      "INFO 06-14 04:15:28 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 06-14 04:15:29 model_runner.py:1110] Starting to load model ibm-granite/granite-3.3-8b-instruct...\n",
      "INFO 06-14 04:15:29 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:06,  2.21s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:04<00:04,  2.25s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:01,  1.56s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:07<00:00,  1.75s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:07<00:00,  1.82s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-14 04:15:37 model_runner.py:1115] Loading model weights took 15.2531 GB\n",
      "INFO 06-14 04:15:37 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 06-14 04:15:41 worker.py:267] Memory profiling takes 4.00 seconds\n",
      "INFO 06-14 04:15:41 worker.py:267] the current vLLM instance can use total_gpu_memory (79.21GiB) x gpu_memory_utilization (0.45) = 35.64GiB\n",
      "INFO 06-14 04:15:41 worker.py:267] model weights take 15.25GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 3.38GiB; the rest of the memory reserved for KV Cache is 16.85GiB.\n",
      "INFO 06-14 04:15:42 executor_base.py:111] # cuda blocks: 6902, # CPU blocks: 1638\n",
      "INFO 06-14 04:15:42 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 3.37x\n",
      "INFO 06-14 04:15:43 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 6.08 seconds\n",
      "INFO 06-14 04:15:46 serving_models.py:174] Loaded new LoRA adapter: name 'local-granite-3.3-8b-lora-rag-context-relevancy', path 'local-granite-3.3-8b-lora-rag-context-relevancy'\n",
      "INFO 06-14 04:15:46 api_server.py:958] Starting vLLM API server on http://0.0.0.0:45109\n",
      "INFO 06-14 04:15:46 launcher.py:23] Available routes are:\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /docs, Methods: HEAD, GET\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /health, Methods: GET\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /ping, Methods: POST, GET\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /tokenize, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /detokenize, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /v1/models, Methods: GET\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /version, Methods: GET\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /v1/completions, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /v1/embeddings, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /pooling, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /score, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /v1/score, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /rerank, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /v1/rerank, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /v2/rerank, Methods: POST\n",
      "INFO 06-14 04:15:46 launcher.py:31] Route: /invocations, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3986647]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:33492 - \"GET /ping HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(\n",
    "        base_model_name, lora_adapters=[(lora_model_name, lora_model_name)]\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    lora_backend = server.make_lora_backend(lora_model_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # Modify the constants here as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    openai_base_model_name = base_model_name\n",
    "    openai_lora_model_name = lora_model_name\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_base_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": openai_lora_model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b186b25-140c-4461-85cf-d5429690c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a short conversation.\n",
    "# Base conversation about pets\n",
    "base_messages = [\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm here to help you prepare for your job interview!\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I have a job interview next week for a marketing manager position.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Congratulations! Marketing manager is an exciting role. How are you feeling about it?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm nervous because I haven't interviewed in years, and this is a big career move for me.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"It's natural to feel nervous, but preparation will help boost your confidence.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What should I expect them to ask about my experience with social media campaigns as a marketing manager?\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635826d2-8b5c-45d2-a243-af5717710ed7",
   "metadata": {},
   "source": [
    "## Relevant Document Context Relevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037208d4-be9d-4269-ae27-16ba96c6132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: RELEVANT document - directly addresses outdoor pets and flea risk\n",
    "chat_input_relevant = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": base_messages,\n",
    "        \"documents\": [{\"text\":\n",
    "        \"Marketing manager interviews often focus on campaign experience and measurable results. \"\n",
    "        \"Expect questions about social media ROI, audience engagement metrics, and conversion rates. \"\n",
    "        \"Prepare specific examples of campaigns you've managed, including budget, timeline, and outcomes. \"\n",
    "        \"Interviewers may ask about your experience with different social media platforms and their unique audiences. \"\n",
    "        \"Be ready to discuss how you measure campaign success and adjust strategies based on performance data. \"\n",
    "        \"Knowledge of current social media trends and emerging platforms demonstrates industry awareness.\"}\n",
    "        ],\n",
    "        \"generate_inputs\": {\"temperature\": 0.0},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0459fdf7-1faf-4ed7-91a8-0222d2a73662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-14 04:15:47 logger.py:39] Received request cmpl-b3358e665ffe46259b6154fbb6b1b07e-0: prompt: '<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday\\'s Date: June 14, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>I\\'m here to help you prepare for your job interview!<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I have a job interview next week for a marketing manager position.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Congratulations! Marketing manager is an exciting role. How are you feeling about it?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I\\'m nervous because I haven\\'t interviewed in years, and this is a big career move for me.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>It\\'s natural to feel nervous, but preparation will help boost your confidence.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>final_user_query<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>document {\"document_id\": \"1\"}<|end_of_role|>\\nMarketing manager interviews often focus on campaign experience and measurable results. Expect questions about social media ROI, audience engagement metrics, and conversion rates. Prepare specific examples of campaigns you\\'ve managed, including budget, timeline, and outcomes. Interviewers may ask about your experience with different social media platforms and their unique audiences. Be ready to discuss how you measure campaign success and adjust strategies based on performance data. Knowledge of current social media trends and emerging platforms demonstrates industry awareness.<|end_of_text|>\\n<|start_of_role|>context_relevance: Analyze the provided document in relation to the final user query from the conversation. Determine if the document contains information that could help answer the final user query. Output \\'relevant\\' if the document contains substantial information directly useful for answering the final user query. Output \\'partially relevant\\' if the document contains some related information that could partially help answer the query, or if you are uncertain about the relevance - err on the side of \\'partially relevant\\' when in doubt. Output \\'irrelevant\\' only if the document clearly contains no information that could help answer the final user query. When uncertain, choose \\'partially relevant\\' rather than \\'irrelevant\\'. Your output should be a JSON structure with the context relevance classification:\\n```json\\n{\\n    \"context_relevance\": \"YOUR_CONTEXT_RELEVANCE_CLASSIFICATION_HERE\"\\n}\\n```<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex='```json\\\\n\\\\{\\\\s*\\\\\"context_relevance\\\\\"\\\\s*:\\\\s*\\\\\"(irrelevant|relevant|partially relevant)\\\\\"\\\\s*\\\\}\\\\n```', choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 35, 38, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 2448, 884, 312, 17247, 19551, 47330, 32, 0, 203, 49152, 17594, 49153, 59, 3464, 2442, 372, 3049, 844, 11656, 436, 1370, 3992, 1426, 1071, 19, 0, 203, 49152, 496, 49153, 59, 1159, 312, 3992, 1426, 1071, 2354, 8209, 436, 312, 43004, 7884, 2747, 32, 0, 203, 49152, 17594, 49153, 609, 44688, 17480, 19, 7778, 8842, 7884, 438, 600, 13489, 37869, 4203, 32, 4971, 884, 844, 42289, 2625, 561, 49, 0, 203, 49152, 496, 49153, 59, 3464, 47428, 36860, 3301, 439, 14562, 1330, 1426, 1071, 318, 328, 11274, 30, 461, 458, 438, 312, 6524, 45378, 5169, 436, 597, 32, 0, 203, 49152, 17594, 49153, 1011, 1182, 19844, 372, 10871, 47428, 36860, 30, 1273, 42357, 1098, 3049, 9760, 1370, 24462, 32, 0, 203, 49152, 496, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 3220, 81, 496, 81, 1217, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 2812, 3447, 2812, 81, 314, 563, 313, 35, 3612, 49153, 203, 4494, 8842, 7884, 1426, 4338, 12270, 9280, 544, 24405, 12409, 461, 15499, 305, 444, 3276, 32, 9177, 10017, 2625, 15332, 7444, 11253, 59, 30, 36021, 14707, 3877, 8607, 30, 461, 11741, 26059, 32, 20920, 2818, 8272, 432, 24405, 101, 844, 4763, 12852, 30, 6237, 23912, 30, 24253, 30, 461, 46197, 32, 4803, 1071, 483, 1631, 7660, 2625, 1370, 12409, 623, 3449, 15332, 7444, 17503, 461, 3623, 5799, 21091, 91, 3015, 32, 4261, 8753, 372, 23273, 2624, 844, 9413, 24405, 3117, 461, 10891, 35678, 4122, 544, 7871, 706, 32, 38401, 432, 1550, 15332, 7444, 31010, 101, 461, 2642, 265, 2563, 17503, 35968, 29404, 18934, 4321, 32, 0, 203, 49152, 1819, 81, 268, 40289, 44, 2244, 12884, 322, 3943, 1825, 328, 14772, 372, 322, 1158, 1256, 2467, 645, 322, 19509, 32, 22022, 415, 322, 1825, 4304, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5582, 330, 30762, 25, 415, 322, 1825, 4304, 14226, 2471, 7029, 6364, 436, 7592, 299, 322, 1158, 1256, 2467, 32, 5582, 330, 2050, 6321, 10088, 25, 415, 322, 1825, 4304, 1629, 5886, 2471, 688, 3065, 34659, 3049, 7592, 322, 2467, 30, 556, 415, 844, 884, 31909, 504, 2625, 322, 316, 40289, 429, 1137, 544, 322, 5209, 432, 330, 2050, 6321, 10088, 25, 1412, 328, 40062, 32, 5582, 330, 476, 30762, 25, 1755, 415, 322, 1825, 27298, 4304, 1289, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5076, 31909, 504, 30, 9173, 330, 2050, 6321, 10088, 25, 9283, 2784, 330, 476, 30762, 2473, 10604, 1688, 1395, 526, 312, 3398, 5193, 623, 322, 1619, 316, 40289, 17717, 44, 203, 914, 1260, 203, 109, 284, 313, 1819, 81, 268, 40289, 563, 313, 25735, 81, 13952, 81, 613, 815, 72, 6307, 81, 25418, 38075, 9590, 81, 4386, 20, 203, 111, 203, 914, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-14 04:15:53 engine.py:280] Added request cmpl-b3358e665ffe46259b6154fbb6b1b07e-0.\n",
      "INFO 06-14 04:15:53 metrics.py:455] Avg prompt throughput: 53.5 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n",
      "INFO:     127.0.0.1:33496 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"relevant\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"tool_calls\": [],\n",
      "  \"reasoning_content\": null,\n",
      "  \"citations\": null,\n",
      "  \"documents\": null,\n",
      "  \"hallucinations\": null,\n",
      "  \"stop_reason\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "io_proc = ContextRelevancyIOProcessor(backend)\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(chat_input_relevant)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e516d73-e191-4611-8ee9-5d2bfad40e97",
   "metadata": {},
   "source": [
    "## Partially Relevant Context Relevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7399d937-9a82-411c-968d-52b4ae6edd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input_partially_relevant = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": base_messages,\n",
    "        \"documents\": [{\"text\":\n",
    "        \"Job interviews typically follow a structured format with behavioral and technical questions. \"\n",
    "        \"Preparing specific examples using the STAR method helps answer behavioral questions effectively. \"\n",
    "        \"Research the company's mission, values, and recent news before your interview. \"\n",
    "        \"Dress appropriately for the company culture and arrive 10-15 minutes early. \"\n",
    "        \"Prepare thoughtful questions to ask the interviewer about the role and company. \"\n",
    "        \"Following up with a thank-you email within 24 hours shows professionalism and interest.\"}\n",
    "        ],\n",
    "        \"generate_inputs\": {\"temperature\": 0.0},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62685c9f-2205-4918-85b9-0fe53570c789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-14 04:15:54 logger.py:39] Received request cmpl-5fb8f1b7c5d64024891c522ee0bfc036-0: prompt: '<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday\\'s Date: June 14, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>I\\'m here to help you prepare for your job interview!<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I have a job interview next week for a marketing manager position.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Congratulations! Marketing manager is an exciting role. How are you feeling about it?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I\\'m nervous because I haven\\'t interviewed in years, and this is a big career move for me.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>It\\'s natural to feel nervous, but preparation will help boost your confidence.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>final_user_query<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>document {\"document_id\": \"1\"}<|end_of_role|>\\nJob interviews typically follow a structured format with behavioral and technical questions. Preparing specific examples using the STAR method helps answer behavioral questions effectively. Research the company\\'s mission, values, and recent news before your interview. Dress appropriately for the company culture and arrive 10-15 minutes early. Prepare thoughtful questions to ask the interviewer about the role and company. Following up with a thank-you email within 24 hours shows professionalism and interest.<|end_of_text|>\\n<|start_of_role|>context_relevance: Analyze the provided document in relation to the final user query from the conversation. Determine if the document contains information that could help answer the final user query. Output \\'relevant\\' if the document contains substantial information directly useful for answering the final user query. Output \\'partially relevant\\' if the document contains some related information that could partially help answer the query, or if you are uncertain about the relevance - err on the side of \\'partially relevant\\' when in doubt. Output \\'irrelevant\\' only if the document clearly contains no information that could help answer the final user query. When uncertain, choose \\'partially relevant\\' rather than \\'irrelevant\\'. Your output should be a JSON structure with the context relevance classification:\\n```json\\n{\\n    \"context_relevance\": \"YOUR_CONTEXT_RELEVANCE_CLASSIFICATION_HERE\"\\n}\\n```<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex='```json\\\\n\\\\{\\\\s*\\\\\"context_relevance\\\\\"\\\\s*:\\\\s*\\\\\"(irrelevant|relevant|partially relevant)\\\\\"\\\\s*\\\\}\\\\n```', choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 35, 38, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 2448, 884, 312, 17247, 19551, 47330, 32, 0, 203, 49152, 17594, 49153, 59, 3464, 2442, 372, 3049, 844, 11656, 436, 1370, 3992, 1426, 1071, 19, 0, 203, 49152, 496, 49153, 59, 1159, 312, 3992, 1426, 1071, 2354, 8209, 436, 312, 43004, 7884, 2747, 32, 0, 203, 49152, 17594, 49153, 609, 44688, 17480, 19, 7778, 8842, 7884, 438, 600, 13489, 37869, 4203, 32, 4971, 884, 844, 42289, 2625, 561, 49, 0, 203, 49152, 496, 49153, 59, 3464, 47428, 36860, 3301, 439, 14562, 1330, 1426, 1071, 318, 328, 11274, 30, 461, 458, 438, 312, 6524, 45378, 5169, 436, 597, 32, 0, 203, 49152, 17594, 49153, 1011, 1182, 19844, 372, 10871, 47428, 36860, 30, 1273, 42357, 1098, 3049, 9760, 1370, 24462, 32, 0, 203, 49152, 496, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 3220, 81, 496, 81, 1217, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 2812, 3447, 2812, 81, 314, 563, 313, 35, 3612, 49153, 203, 3782, 1426, 4338, 18214, 1976, 312, 28546, 2179, 623, 5853, 279, 461, 22131, 10017, 32, 7474, 26124, 2818, 8272, 1471, 322, 3849, 783, 1411, 15761, 7592, 5853, 279, 10017, 29419, 32, 18699, 322, 10512, 1182, 28860, 30, 2078, 30, 461, 8289, 14165, 2670, 1370, 1426, 1071, 32, 475, 714, 36808, 436, 322, 10512, 27668, 461, 2099, 5864, 225, 35, 34, 31, 35, 39, 10135, 15883, 32, 20920, 10889, 2790, 10017, 372, 7660, 322, 1426, 11429, 2625, 322, 4203, 461, 10512, 32, 41339, 973, 623, 312, 17715, 31, 9619, 4096, 4797, 225, 36, 38, 11632, 9830, 39073, 5738, 461, 10219, 32, 0, 203, 49152, 1819, 81, 268, 40289, 44, 2244, 12884, 322, 3943, 1825, 328, 14772, 372, 322, 1158, 1256, 2467, 645, 322, 19509, 32, 22022, 415, 322, 1825, 4304, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5582, 330, 30762, 25, 415, 322, 1825, 4304, 14226, 2471, 7029, 6364, 436, 7592, 299, 322, 1158, 1256, 2467, 32, 5582, 330, 2050, 6321, 10088, 25, 415, 322, 1825, 4304, 1629, 5886, 2471, 688, 3065, 34659, 3049, 7592, 322, 2467, 30, 556, 415, 844, 884, 31909, 504, 2625, 322, 316, 40289, 429, 1137, 544, 322, 5209, 432, 330, 2050, 6321, 10088, 25, 1412, 328, 40062, 32, 5582, 330, 476, 30762, 25, 1755, 415, 322, 1825, 27298, 4304, 1289, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5076, 31909, 504, 30, 9173, 330, 2050, 6321, 10088, 25, 9283, 2784, 330, 476, 30762, 2473, 10604, 1688, 1395, 526, 312, 3398, 5193, 623, 322, 1619, 316, 40289, 17717, 44, 203, 914, 1260, 203, 109, 284, 313, 1819, 81, 268, 40289, 563, 313, 25735, 81, 13952, 81, 613, 815, 72, 6307, 81, 25418, 38075, 9590, 81, 4386, 20, 203, 111, 203, 914, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-14 04:15:54 engine.py:280] Added request cmpl-5fb8f1b7c5d64024891c522ee0bfc036-0.\n",
      "INFO:     127.0.0.1:33496 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"partially relevant\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"tool_calls\": [],\n",
      "  \"reasoning_content\": null,\n",
      "  \"citations\": null,\n",
      "  \"documents\": null,\n",
      "  \"hallucinations\": null,\n",
      "  \"stop_reason\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "io_proc = ContextRelevancyIOProcessor(backend)\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(chat_input_partially_relevant)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409286af-97d8-4e5e-aaae-c119467bb581",
   "metadata": {},
   "source": [
    "## Irrelevant Context Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d050eff-8f7b-4fe3-b77c-3564bce5c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input_irrelevant = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": base_messages,\n",
    "        \"documents\": [{\"text\":\n",
    "        \"Proper knife skills are fundamental to efficient cooking and food safety in the kitchen. \"\n",
    "        \"Different cuts like julienne, brunoise, and chiffonade serve specific culinary purposes. \"\n",
    "        \"Sharp knives are actually safer than dull ones because they require less pressure to cut. \"\n",
    "        \"Learning to properly hold and control a chef's knife takes practice and patience. \"\n",
    "        \"Professional chefs can prep vegetables much faster due to their refined knife techniques. \"\n",
    "        \"Regular knife maintenance including sharpening and proper storage extends blade life.\"\n",
    "                }\n",
    "        ],\n",
    "        \"generate_inputs\": {\"temperature\": 0.0},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d94d73-feeb-452b-ba59-cbc6c7714bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-14 04:15:55 logger.py:39] Received request cmpl-0f3e067a374b4f9095d0c5e8a321355d-0: prompt: '<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday\\'s Date: June 14, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>I\\'m here to help you prepare for your job interview!<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I have a job interview next week for a marketing manager position.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Congratulations! Marketing manager is an exciting role. How are you feeling about it?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I\\'m nervous because I haven\\'t interviewed in years, and this is a big career move for me.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>It\\'s natural to feel nervous, but preparation will help boost your confidence.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>final_user_query<|end_of_role|>What should I expect them to ask about my experience with social media campaigns as a marketing manager?<|end_of_text|>\\n<|start_of_role|>document {\"document_id\": \"1\"}<|end_of_role|>\\nProper knife skills are fundamental to efficient cooking and food safety in the kitchen. Different cuts like julienne, brunoise, and chiffonade serve specific culinary purposes. Sharp knives are actually safer than dull ones because they require less pressure to cut. Learning to properly hold and control a chef\\'s knife takes practice and patience. Professional chefs can prep vegetables much faster due to their refined knife techniques. Regular knife maintenance including sharpening and proper storage extends blade life.<|end_of_text|>\\n<|start_of_role|>context_relevance: Analyze the provided document in relation to the final user query from the conversation. Determine if the document contains information that could help answer the final user query. Output \\'relevant\\' if the document contains substantial information directly useful for answering the final user query. Output \\'partially relevant\\' if the document contains some related information that could partially help answer the query, or if you are uncertain about the relevance - err on the side of \\'partially relevant\\' when in doubt. Output \\'irrelevant\\' only if the document clearly contains no information that could help answer the final user query. When uncertain, choose \\'partially relevant\\' rather than \\'irrelevant\\'. Your output should be a JSON structure with the context relevance classification:\\n```json\\n{\\n    \"context_relevance\": \"YOUR_CONTEXT_RELEVANCE_CLASSIFICATION_HERE\"\\n}\\n```<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex='```json\\\\n\\\\{\\\\s*\\\\\"context_relevance\\\\\"\\\\s*:\\\\s*\\\\\"(irrelevant|relevant|partially relevant)\\\\\"\\\\s*\\\\}\\\\n```', choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 35, 38, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 2448, 884, 312, 17247, 19551, 47330, 32, 0, 203, 49152, 17594, 49153, 59, 3464, 2442, 372, 3049, 844, 11656, 436, 1370, 3992, 1426, 1071, 19, 0, 203, 49152, 496, 49153, 59, 1159, 312, 3992, 1426, 1071, 2354, 8209, 436, 312, 43004, 7884, 2747, 32, 0, 203, 49152, 17594, 49153, 609, 44688, 17480, 19, 7778, 8842, 7884, 438, 600, 13489, 37869, 4203, 32, 4971, 884, 844, 42289, 2625, 561, 49, 0, 203, 49152, 496, 49153, 59, 3464, 47428, 36860, 3301, 439, 14562, 1330, 1426, 1071, 318, 328, 11274, 30, 461, 458, 438, 312, 6524, 45378, 5169, 436, 597, 32, 0, 203, 49152, 17594, 49153, 1011, 1182, 19844, 372, 10871, 47428, 36860, 30, 1273, 42357, 1098, 3049, 9760, 1370, 24462, 32, 0, 203, 49152, 496, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 3220, 81, 496, 81, 1217, 49153, 8197, 1395, 439, 1424, 3026, 372, 7660, 2625, 1672, 12409, 623, 15332, 7444, 24405, 101, 619, 312, 43004, 7884, 49, 0, 203, 49152, 2812, 3447, 2812, 81, 314, 563, 313, 35, 3612, 49153, 203, 590, 462, 22851, 6354, 22728, 884, 40250, 372, 17457, 23682, 299, 461, 17902, 26160, 328, 322, 831, 30447, 32, 43044, 11909, 101, 2124, 594, 338, 8605, 941, 30, 323, 1945, 97, 1125, 30, 461, 663, 2937, 267, 1525, 13942, 2818, 31334, 3172, 14998, 32, 44937, 22851, 4119, 884, 6723, 4257, 1012, 2784, 343, 514, 11668, 3301, 2953, 1205, 6679, 23319, 372, 11909, 32, 14292, 372, 10273, 7629, 461, 3498, 312, 44051, 1182, 22851, 6354, 8727, 18572, 461, 8402, 5467, 32, 45530, 8277, 2548, 883, 35535, 5292, 371, 6975, 5610, 15162, 7254, 372, 3623, 2488, 1441, 22851, 6354, 26114, 32, 33196, 22851, 6354, 24514, 6237, 787, 282, 1127, 299, 461, 7240, 4759, 2026, 3145, 1525, 12535, 32, 0, 203, 49152, 1819, 81, 268, 40289, 44, 2244, 12884, 322, 3943, 1825, 328, 14772, 372, 322, 1158, 1256, 2467, 645, 322, 19509, 32, 22022, 415, 322, 1825, 4304, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5582, 330, 30762, 25, 415, 322, 1825, 4304, 14226, 2471, 7029, 6364, 436, 7592, 299, 322, 1158, 1256, 2467, 32, 5582, 330, 2050, 6321, 10088, 25, 415, 322, 1825, 4304, 1629, 5886, 2471, 688, 3065, 34659, 3049, 7592, 322, 2467, 30, 556, 415, 844, 884, 31909, 504, 2625, 322, 316, 40289, 429, 1137, 544, 322, 5209, 432, 330, 2050, 6321, 10088, 25, 1412, 328, 40062, 32, 5582, 330, 476, 30762, 25, 1755, 415, 322, 1825, 27298, 4304, 1289, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5076, 31909, 504, 30, 9173, 330, 2050, 6321, 10088, 25, 9283, 2784, 330, 476, 30762, 2473, 10604, 1688, 1395, 526, 312, 3398, 5193, 623, 322, 1619, 316, 40289, 17717, 44, 203, 914, 1260, 203, 109, 284, 313, 1819, 81, 268, 40289, 563, 313, 25735, 81, 13952, 81, 613, 815, 72, 6307, 81, 25418, 38075, 9590, 81, 4386, 20, 203, 111, 203, 914, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-14 04:15:55 engine.py:280] Added request cmpl-0f3e067a374b4f9095d0c5e8a321355d-0.\n",
      "INFO:     127.0.0.1:33496 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"irrelevant\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"tool_calls\": [],\n",
      "  \"reasoning_content\": null,\n",
      "  \"citations\": null,\n",
      "  \"documents\": null,\n",
      "  \"hallucinations\": null,\n",
      "  \"stop_reason\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "io_proc = ContextRelevancyIOProcessor(backend)\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(chat_input_irrelevant)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17bfafc-9877-41cb-985c-d6b693c4126c",
   "metadata": {},
   "source": [
    "## Additional Example on Gardening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc6fd58-4a56-4119-860d-90aa615eca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gardening_messages = [\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I'd love to help with your gardening questions.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I just started a vegetable garden in my backyard this spring.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": \"That's wonderful! What vegetables are you growing?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I planted tomatoes, peppers, and some lettuce. Everything was doing great until recently.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Oh no, what's been happening with your plants?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The tomato leaves are turning yellow and dropping off. Is this a disease?\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc7370-5e0a-4ee7-9def-7b7e65e30656",
   "metadata": {},
   "source": [
    "### Gardening Example With Relevant Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e0c6e1e-d0a1-4b34-833d-22c5d6527c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gardening_relevant = Granite3Point3Inputs.model_validate({\n",
    "    \"messages\": gardening_messages,\n",
    "    \"documents\": [{\"text\":\n",
    "        \"Yellow leaves on tomato plants can indicate several diseases or conditions. \"\n",
    "        \"Early blight causes yellowing leaves that develop brown spots and eventually drop off. \"\n",
    "        \"Fusarium wilt starts with yellowing of lower leaves and progresses upward. \"\n",
    "        \"Overwatering can also cause yellowing as roots become waterlogged and unable to absorb nutrients. \"\n",
    "        \"Nitrogen deficiency typically shows as yellowing starting from the bottom leaves. \"\n",
    "        \"Proper diagnosis requires examining the pattern of yellowing and any accompanying symptoms.\"}\n",
    "    ],\n",
    "    \"generate_inputs\": {\"temperature\": 0.0},\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0e202e2-b443-43f2-a4d2-97eb45258f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-14 04:15:56 logger.py:39] Received request cmpl-be99205545b648949dc737c48118764e-0: prompt: '<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday\\'s Date: June 14, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Hi! I\\'d love to help with your gardening questions.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I just started a vegetable garden in my backyard this spring.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>That\\'s wonderful! What vegetables are you growing?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I planted tomatoes, peppers, and some lettuce. Everything was doing great until recently.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Oh no, what\\'s been happening with your plants?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>The tomato leaves are turning yellow and dropping off. Is this a disease?<|end_of_text|>\\n<|start_of_role|>final_user_query<|end_of_role|>The tomato leaves are turning yellow and dropping off. Is this a disease?<|end_of_text|>\\n<|start_of_role|>document {\"document_id\": \"1\"}<|end_of_role|>\\nYellow leaves on tomato plants can indicate several diseases or conditions. Early blight causes yellowing leaves that develop brown spots and eventually drop off. Fusarium wilt starts with yellowing of lower leaves and progresses upward. Overwatering can also cause yellowing as roots become waterlogged and unable to absorb nutrients. Nitrogen deficiency typically shows as yellowing starting from the bottom leaves. Proper diagnosis requires examining the pattern of yellowing and any accompanying symptoms.<|end_of_text|>\\n<|start_of_role|>context_relevance: Analyze the provided document in relation to the final user query from the conversation. Determine if the document contains information that could help answer the final user query. Output \\'relevant\\' if the document contains substantial information directly useful for answering the final user query. Output \\'partially relevant\\' if the document contains some related information that could partially help answer the query, or if you are uncertain about the relevance - err on the side of \\'partially relevant\\' when in doubt. Output \\'irrelevant\\' only if the document clearly contains no information that could help answer the final user query. When uncertain, choose \\'partially relevant\\' rather than \\'irrelevant\\'. Your output should be a JSON structure with the context relevance classification:\\n```json\\n{\\n    \"context_relevance\": \"YOUR_CONTEXT_RELEVANCE_CLASSIFICATION_HERE\"\\n}\\n```<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex='```json\\\\n\\\\{\\\\s*\\\\\"context_relevance\\\\\"\\\\s*:\\\\s*\\\\\"(irrelevant|relevant|partially relevant)\\\\\"\\\\s*\\\\}\\\\n```', choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 35, 38, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 2448, 884, 312, 17247, 19551, 47330, 32, 0, 203, 49152, 17594, 49153, 12575, 19, 439, 5438, 14290, 372, 3049, 623, 1370, 485, 704, 10876, 10017, 32, 0, 203, 49152, 496, 49153, 59, 2258, 7307, 312, 5292, 371, 444, 485, 22461, 328, 1672, 1751, 40937, 458, 22748, 32, 0, 203, 49152, 17594, 49153, 4974, 1182, 16741, 2790, 19, 6180, 5292, 371, 6975, 884, 844, 42195, 49, 0, 203, 49152, 496, 49153, 59, 1278, 8490, 372, 1287, 37764, 30, 44636, 3351, 30, 461, 1629, 1207, 102, 6502, 32, 35838, 1597, 8113, 8040, 6222, 18821, 32, 0, 203, 49152, 17594, 49153, 24628, 1289, 30, 2769, 1182, 2583, 20164, 623, 1370, 1278, 2376, 49, 0, 203, 49152, 496, 49153, 1318, 372, 1287, 97, 26243, 884, 38740, 28506, 461, 43757, 2126, 32, 3611, 458, 312, 35293, 49, 0, 203, 49152, 3220, 81, 496, 81, 1217, 49153, 1318, 372, 1287, 97, 26243, 884, 38740, 28506, 461, 43757, 2126, 32, 3611, 458, 312, 35293, 49, 0, 203, 49152, 2812, 3447, 2812, 81, 314, 563, 313, 35, 3612, 49153, 203, 31257, 26243, 544, 372, 1287, 97, 1278, 2376, 883, 14190, 10204, 36320, 5633, 556, 5479, 32, 45647, 323, 2429, 15663, 28506, 299, 26243, 688, 10227, 40221, 1869, 13800, 461, 23840, 6278, 2126, 32, 506, 352, 2807, 378, 341, 3413, 11187, 623, 28506, 299, 432, 7216, 26243, 461, 23161, 6071, 973, 2489, 32, 9846, 12475, 299, 883, 2329, 5749, 28506, 299, 619, 33032, 9555, 13408, 17872, 461, 17049, 372, 46522, 84, 42422, 9907, 101, 32, 489, 44162, 2518, 665, 47128, 18214, 9830, 619, 28506, 299, 8049, 645, 322, 6822, 26243, 32, 1312, 462, 2068, 31912, 7171, 538, 13493, 299, 322, 5257, 432, 28506, 299, 461, 1346, 22188, 11385, 39171, 32, 0, 203, 49152, 1819, 81, 268, 40289, 44, 2244, 12884, 322, 3943, 1825, 328, 14772, 372, 322, 1158, 1256, 2467, 645, 322, 19509, 32, 22022, 415, 322, 1825, 4304, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5582, 330, 30762, 25, 415, 322, 1825, 4304, 14226, 2471, 7029, 6364, 436, 7592, 299, 322, 1158, 1256, 2467, 32, 5582, 330, 2050, 6321, 10088, 25, 415, 322, 1825, 4304, 1629, 5886, 2471, 688, 3065, 34659, 3049, 7592, 322, 2467, 30, 556, 415, 844, 884, 31909, 504, 2625, 322, 316, 40289, 429, 1137, 544, 322, 5209, 432, 330, 2050, 6321, 10088, 25, 1412, 328, 40062, 32, 5582, 330, 476, 30762, 25, 1755, 415, 322, 1825, 27298, 4304, 1289, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5076, 31909, 504, 30, 9173, 330, 2050, 6321, 10088, 25, 9283, 2784, 330, 476, 30762, 2473, 10604, 1688, 1395, 526, 312, 3398, 5193, 623, 322, 1619, 316, 40289, 17717, 44, 203, 914, 1260, 203, 109, 284, 313, 1819, 81, 268, 40289, 563, 313, 25735, 81, 13952, 81, 613, 815, 72, 6307, 81, 25418, 38075, 9590, 81, 4386, 20, 203, 111, 203, 914, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-14 04:15:56 engine.py:280] Added request cmpl-be99205545b648949dc737c48118764e-0.\n",
      "INFO:     127.0.0.1:33496 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"relevant\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"tool_calls\": [],\n",
      "  \"reasoning_content\": null,\n",
      "  \"citations\": null,\n",
      "  \"documents\": null,\n",
      "  \"hallucinations\": null,\n",
      "  \"stop_reason\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "io_proc = ContextRelevancyIOProcessor(backend)\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(gardening_relevant)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41db5e1-6f40-4117-a4be-88a2f29f00b5",
   "metadata": {},
   "source": [
    "### Gardening Example with Irrelevant Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a5ac5f-38db-4278-9250-2c50d7cfa4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gardening_irrelevant = Granite3Point3Inputs.model_validate({\n",
    "    \"messages\": gardening_messages,\n",
    "    \"documents\": [{\"text\":\n",
    "        \"Restoring antique furniture requires careful assessment of the wood type and existing finish. \"\n",
    "        \"Stripping old paint or varnish should be done in a well-ventilated area with proper safety equipment. \"\n",
    "        \"Sanding between coats ensures a smooth final finish on wooden surfaces. \"\n",
    "        \"Wood stain penetrates deeper than paint and highlights the natural grain patterns. \"\n",
    "        \"Professional restoration can increase the value of valuable antique pieces. \"\n",
    "        \"Regular maintenance with appropriate wood polish helps preserve restored furniture.\"}\n",
    "    ],\n",
    "    \"generate_inputs\": {\"temperature\": 0.0},\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1be16b6-a58a-427f-88ab-284028cce07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-14 04:15:57 logger.py:39] Received request cmpl-5c3dfcf52cbd4f668d60238a32776c2c-0: prompt: '<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday\\'s Date: June 14, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Hi! I\\'d love to help with your gardening questions.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I just started a vegetable garden in my backyard this spring.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>That\\'s wonderful! What vegetables are you growing?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I planted tomatoes, peppers, and some lettuce. Everything was doing great until recently.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Oh no, what\\'s been happening with your plants?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>The tomato leaves are turning yellow and dropping off. Is this a disease?<|end_of_text|>\\n<|start_of_role|>final_user_query<|end_of_role|>The tomato leaves are turning yellow and dropping off. Is this a disease?<|end_of_text|>\\n<|start_of_role|>document {\"document_id\": \"1\"}<|end_of_role|>\\nRestoring antique furniture requires careful assessment of the wood type and existing finish. Stripping old paint or varnish should be done in a well-ventilated area with proper safety equipment. Sanding between coats ensures a smooth final finish on wooden surfaces. Wood stain penetrates deeper than paint and highlights the natural grain patterns. Professional restoration can increase the value of valuable antique pieces. Regular maintenance with appropriate wood polish helps preserve restored furniture.<|end_of_text|>\\n<|start_of_role|>context_relevance: Analyze the provided document in relation to the final user query from the conversation. Determine if the document contains information that could help answer the final user query. Output \\'relevant\\' if the document contains substantial information directly useful for answering the final user query. Output \\'partially relevant\\' if the document contains some related information that could partially help answer the query, or if you are uncertain about the relevance - err on the side of \\'partially relevant\\' when in doubt. Output \\'irrelevant\\' only if the document clearly contains no information that could help answer the final user query. When uncertain, choose \\'partially relevant\\' rather than \\'irrelevant\\'. Your output should be a JSON structure with the context relevance classification:\\n```json\\n{\\n    \"context_relevance\": \"YOUR_CONTEXT_RELEVANCE_CLASSIFICATION_HERE\"\\n}\\n```<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex='```json\\\\n\\\\{\\\\s*\\\\\"context_relevance\\\\\"\\\\s*:\\\\s*\\\\\"(irrelevant|relevant|partially relevant)\\\\\"\\\\s*\\\\}\\\\n```', choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 35, 38, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 2448, 884, 312, 17247, 19551, 47330, 32, 0, 203, 49152, 17594, 49153, 12575, 19, 439, 5438, 14290, 372, 3049, 623, 1370, 485, 704, 10876, 10017, 32, 0, 203, 49152, 496, 49153, 59, 2258, 7307, 312, 5292, 371, 444, 485, 22461, 328, 1672, 1751, 40937, 458, 22748, 32, 0, 203, 49152, 17594, 49153, 4974, 1182, 16741, 2790, 19, 6180, 5292, 371, 6975, 884, 844, 42195, 49, 0, 203, 49152, 496, 49153, 59, 1278, 8490, 372, 1287, 37764, 30, 44636, 3351, 30, 461, 1629, 1207, 102, 6502, 32, 35838, 1597, 8113, 8040, 6222, 18821, 32, 0, 203, 49152, 17594, 49153, 24628, 1289, 30, 2769, 1182, 2583, 20164, 623, 1370, 1278, 2376, 49, 0, 203, 49152, 496, 49153, 1318, 372, 1287, 97, 26243, 884, 38740, 28506, 461, 43757, 2126, 32, 3611, 458, 312, 35293, 49, 0, 203, 49152, 3220, 81, 496, 81, 1217, 49153, 1318, 372, 1287, 97, 26243, 884, 38740, 28506, 461, 43757, 2126, 32, 3611, 458, 312, 35293, 49, 0, 203, 49152, 2812, 3447, 2812, 81, 314, 563, 313, 35, 3612, 49153, 203, 6996, 8696, 17112, 2673, 296, 47601, 7171, 34942, 34154, 432, 322, 41878, 827, 461, 4905, 11361, 32, 30603, 11747, 3610, 16880, 556, 773, 45054, 1395, 526, 3390, 328, 312, 4487, 31, 654, 335, 733, 5951, 623, 7240, 26160, 31454, 32, 42201, 299, 3733, 2098, 1966, 26091, 312, 19052, 1158, 11361, 544, 41878, 272, 43632, 32, 624, 4907, 376, 504, 298, 2106, 359, 1196, 43396, 2784, 16880, 461, 8839, 101, 322, 19844, 2914, 504, 15103, 32, 45530, 6109, 271, 367, 883, 12075, 322, 786, 432, 45328, 17112, 2673, 23073, 32, 33196, 24514, 623, 9136, 41878, 7743, 1708, 15761, 24070, 34264, 296, 47601, 32, 0, 203, 49152, 1819, 81, 268, 40289, 44, 2244, 12884, 322, 3943, 1825, 328, 14772, 372, 322, 1158, 1256, 2467, 645, 322, 19509, 32, 22022, 415, 322, 1825, 4304, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5582, 330, 30762, 25, 415, 322, 1825, 4304, 14226, 2471, 7029, 6364, 436, 7592, 299, 322, 1158, 1256, 2467, 32, 5582, 330, 2050, 6321, 10088, 25, 415, 322, 1825, 4304, 1629, 5886, 2471, 688, 3065, 34659, 3049, 7592, 322, 2467, 30, 556, 415, 844, 884, 31909, 504, 2625, 322, 316, 40289, 429, 1137, 544, 322, 5209, 432, 330, 2050, 6321, 10088, 25, 1412, 328, 40062, 32, 5582, 330, 476, 30762, 25, 1755, 415, 322, 1825, 27298, 4304, 1289, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5076, 31909, 504, 30, 9173, 330, 2050, 6321, 10088, 25, 9283, 2784, 330, 476, 30762, 2473, 10604, 1688, 1395, 526, 312, 3398, 5193, 623, 322, 1619, 316, 40289, 17717, 44, 203, 914, 1260, 203, 109, 284, 313, 1819, 81, 268, 40289, 563, 313, 25735, 81, 13952, 81, 613, 815, 72, 6307, 81, 25418, 38075, 9590, 81, 4386, 20, 203, 111, 203, 914, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-14 04:15:57 engine.py:280] Added request cmpl-5c3dfcf52cbd4f668d60238a32776c2c-0.\n",
      "INFO:     127.0.0.1:33496 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"irrelevant\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"tool_calls\": [],\n",
      "  \"reasoning_content\": null,\n",
      "  \"citations\": null,\n",
      "  \"documents\": null,\n",
      "  \"hallucinations\": null,\n",
      "  \"stop_reason\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "io_proc = ContextRelevancyIOProcessor(backend)\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(gardening_irrelevant)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807228a7-a02d-44fb-aa4c-291333b63328",
   "metadata": {},
   "source": [
    "### Gardening Example With Partially Relevant Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa92f1a3-df8a-48fa-80cf-688603fec0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gardening_partial = Granite3Point3Inputs.model_validate({\n",
    "    \"messages\": gardening_messages,\n",
    "    \"documents\": [{\"text\":\n",
    "        \"Successful vegetable gardening requires attention to soil quality, watering, and plant spacing. \"\n",
    "        \"Different vegetables have varying sunlight and water requirements throughout the growing season. \"\n",
    "        \"Regular inspection of plants helps identify potential problems before they become serious. \"\n",
    "        \"Healthy soil with good drainage supports strong root development in all garden plants. \"\n",
    "        \"Crop rotation prevents soil depletion and reduces disease buildup in garden beds. \"\n",
    "        \"Organic mulch helps retain moisture and suppress weeds around vegetable plants.\"}\n",
    "    ],\n",
    "    \"generate_inputs\": {\"temperature\": 0.0},\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47636498-066c-4f5f-91cf-f59b7306df22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-14 04:15:58 logger.py:39] Received request cmpl-1c1ef02b624c4a3f96266e5de03fa433-0: prompt: '<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday\\'s Date: June 14, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Hi! I\\'d love to help with your gardening questions.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I just started a vegetable garden in my backyard this spring.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>That\\'s wonderful! What vegetables are you growing?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>I planted tomatoes, peppers, and some lettuce. Everything was doing great until recently.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>Oh no, what\\'s been happening with your plants?<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>The tomato leaves are turning yellow and dropping off. Is this a disease?<|end_of_text|>\\n<|start_of_role|>final_user_query<|end_of_role|>The tomato leaves are turning yellow and dropping off. Is this a disease?<|end_of_text|>\\n<|start_of_role|>document {\"document_id\": \"1\"}<|end_of_role|>\\nSuccessful vegetable gardening requires attention to soil quality, watering, and plant spacing. Different vegetables have varying sunlight and water requirements throughout the growing season. Regular inspection of plants helps identify potential problems before they become serious. Healthy soil with good drainage supports strong root development in all garden plants. Crop rotation prevents soil depletion and reduces disease buildup in garden beds. Organic mulch helps retain moisture and suppress weeds around vegetable plants.<|end_of_text|>\\n<|start_of_role|>context_relevance: Analyze the provided document in relation to the final user query from the conversation. Determine if the document contains information that could help answer the final user query. Output \\'relevant\\' if the document contains substantial information directly useful for answering the final user query. Output \\'partially relevant\\' if the document contains some related information that could partially help answer the query, or if you are uncertain about the relevance - err on the side of \\'partially relevant\\' when in doubt. Output \\'irrelevant\\' only if the document clearly contains no information that could help answer the final user query. When uncertain, choose \\'partially relevant\\' rather than \\'irrelevant\\'. Your output should be a JSON structure with the context relevance classification:\\n```json\\n{\\n    \"context_relevance\": \"YOUR_CONTEXT_RELEVANCE_CLASSIFICATION_HERE\"\\n}\\n```<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex='```json\\\\n\\\\{\\\\s*\\\\\"context_relevance\\\\\"\\\\s*:\\\\s*\\\\\"(irrelevant|relevant|partially relevant)\\\\\"\\\\s*\\\\}\\\\n```', choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 35, 38, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 2448, 884, 312, 17247, 19551, 47330, 32, 0, 203, 49152, 17594, 49153, 12575, 19, 439, 5438, 14290, 372, 3049, 623, 1370, 485, 704, 10876, 10017, 32, 0, 203, 49152, 496, 49153, 59, 2258, 7307, 312, 5292, 371, 444, 485, 22461, 328, 1672, 1751, 40937, 458, 22748, 32, 0, 203, 49152, 17594, 49153, 4974, 1182, 16741, 2790, 19, 6180, 5292, 371, 6975, 884, 844, 42195, 49, 0, 203, 49152, 496, 49153, 59, 1278, 8490, 372, 1287, 37764, 30, 44636, 3351, 30, 461, 1629, 1207, 102, 6502, 32, 35838, 1597, 8113, 8040, 6222, 18821, 32, 0, 203, 49152, 17594, 49153, 24628, 1289, 30, 2769, 1182, 2583, 20164, 623, 1370, 1278, 2376, 49, 0, 203, 49152, 496, 49153, 1318, 372, 1287, 97, 26243, 884, 38740, 28506, 461, 43757, 2126, 32, 3611, 458, 312, 35293, 49, 0, 203, 49152, 3220, 81, 496, 81, 1217, 49153, 1318, 372, 1287, 97, 26243, 884, 38740, 28506, 461, 43757, 2126, 32, 3611, 458, 312, 35293, 49, 0, 203, 49152, 2812, 3447, 2812, 81, 314, 563, 313, 35, 3612, 49153, 203, 21525, 5292, 371, 444, 485, 704, 10876, 7171, 18885, 372, 1259, 335, 13090, 30, 13408, 299, 30, 461, 26795, 20453, 32, 43044, 5292, 371, 6975, 1159, 31155, 15323, 2429, 461, 13408, 10130, 28025, 322, 42195, 26822, 32, 33196, 47775, 432, 1278, 2376, 15761, 12411, 13792, 9808, 2670, 2953, 9555, 31226, 32, 19256, 107, 1259, 335, 623, 4644, 44262, 381, 9567, 12101, 3051, 8226, 328, 1169, 485, 22461, 1278, 2376, 32, 390, 1962, 11410, 29047, 1259, 335, 9143, 5199, 461, 42036, 35293, 1968, 417, 328, 485, 22461, 23065, 101, 32, 28172, 295, 17476, 380, 15761, 13771, 7562, 427, 513, 461, 28182, 996, 43740, 6835, 5292, 371, 444, 1278, 2376, 32, 0, 203, 49152, 1819, 81, 268, 40289, 44, 2244, 12884, 322, 3943, 1825, 328, 14772, 372, 322, 1158, 1256, 2467, 645, 322, 19509, 32, 22022, 415, 322, 1825, 4304, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5582, 330, 30762, 25, 415, 322, 1825, 4304, 14226, 2471, 7029, 6364, 436, 7592, 299, 322, 1158, 1256, 2467, 32, 5582, 330, 2050, 6321, 10088, 25, 415, 322, 1825, 4304, 1629, 5886, 2471, 688, 3065, 34659, 3049, 7592, 322, 2467, 30, 556, 415, 844, 884, 31909, 504, 2625, 322, 316, 40289, 429, 1137, 544, 322, 5209, 432, 330, 2050, 6321, 10088, 25, 1412, 328, 40062, 32, 5582, 330, 476, 30762, 25, 1755, 415, 322, 1825, 27298, 4304, 1289, 2471, 688, 3065, 3049, 7592, 322, 1158, 1256, 2467, 32, 5076, 31909, 504, 30, 9173, 330, 2050, 6321, 10088, 25, 9283, 2784, 330, 476, 30762, 2473, 10604, 1688, 1395, 526, 312, 3398, 5193, 623, 322, 1619, 316, 40289, 17717, 44, 203, 914, 1260, 203, 109, 284, 313, 1819, 81, 268, 40289, 563, 313, 25735, 81, 13952, 81, 613, 815, 72, 6307, 81, 25418, 38075, 9590, 81, 4386, 20, 203, 111, 203, 914, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-14 04:15:58 engine.py:280] Added request cmpl-1c1ef02b624c4a3f96266e5de03fa433-0.\n",
      "INFO 06-14 04:15:58 metrics.py:455] Avg prompt throughput: 513.1 tokens/s, Avg generation throughput: 20.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n",
      "INFO:     127.0.0.1:33496 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"partially relevant\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"tool_calls\": [],\n",
      "  \"reasoning_content\": null,\n",
      "  \"citations\": null,\n",
      "  \"documents\": null,\n",
      "  \"hallucinations\": null,\n",
      "  \"stop_reason\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "io_proc = ContextRelevancyIOProcessor(backend)\n",
    "# Pass our example input through the I/O processor and retrieve the result\n",
    "chat_result = await io_proc.acreate_chat_completion(gardening_partial)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb82e4-dd42-4648-82df-7c8cf0b8c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU resources\n",
    "if \"server\" in locals():\n",
    "    server.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
