{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end demo with MTRAG benchmark data\n",
    "\n",
    "This notebook shows several examples of end-to-end RAG use cases that use the retrieval\n",
    "IO processor in conjunction with the IO processors for other Granite-based LoRA \n",
    "adapters.\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "models on your own server. To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants in the cell marked\n",
    "`# Constants go here`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from granite_io.io.granite_3_2.input_processors.granite_3_2_input_processor import (\n",
    "    Granite3Point2Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from granite_io.io.base import RewriteRequestProcessor\n",
    "from granite_io.io.retrieval.util import download_mtrag_embeddings\n",
    "from granite_io.io.retrieval import InMemoryRetriever, RetrievalRequestProcessor\n",
    "from granite_io.io.answerability import (\n",
    "    AnswerabilityIOProcessor,\n",
    "    AnswerabilityCompositeIOProcessor,\n",
    ")\n",
    "from granite_io.io.query_rewrite import QueryRewriteIOProcessor\n",
    "from granite_io.io.citations import CitationsCompositeIOProcessor\n",
    "from granite_io.io.hallucinations import HallucinationsCompositeIOProcessor\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "temp_data_dir = \"../data/test_retrieval_temp\"\n",
    "corpus_name = \"govt\"\n",
    "embeddings_data_file = pathlib.Path(temp_data_dir) / f\"{corpus_name}_embeds.parquet\"\n",
    "embedding_model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "model_name = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "query_rewrite_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-query-rewrite\"\n",
    "citations_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-citation-generation\"\n",
    "answerability_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-answerability-prediction\"\n",
    "hallucination_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-hallucination-detection\"\n",
    "\n",
    "run_server = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(\n",
    "        model_name,\n",
    "        lora_adapters=[\n",
    "            (lora_name, lora_name)\n",
    "            for lora_name in (\n",
    "                query_rewrite_lora_name,\n",
    "                citations_lora_name,\n",
    "                answerability_lora_name,\n",
    "                hallucination_lora_name,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    query_rewrite_lora_backend = server.make_lora_backend(query_rewrite_lora_name)\n",
    "    citations_lora_backend = server.make_lora_backend(citations_lora_name)\n",
    "    answerability_lora_backend = server.make_lora_backend(answerability_lora_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # The constants here are for the server that local_vllm_server.ipynb starts.\n",
    "    # Modify as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    query_rewrite_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": query_rewrite_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    citations_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": citations_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    answerability_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": answerability_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    hallucination_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": hallucination_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the indexed corpus if it hasn't already been downloaded.\n",
    "# This notebook uses a subset of the government corpus from the MTRAG benchmark.\n",
    "embeddings_location = f\"{temp_data_dir}/{corpus_name}_embeds.parquet\"\n",
    "if not os.path.exists(embeddings_location):\n",
    "    download_mtrag_embeddings(embedding_model_name, corpus_name, embeddings_location)\n",
    "embeddings_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating an example chat completion request. \n",
    "\n",
    "*TODO: Insert description of the chat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completions request.\n",
    "chat_input = Granite3Point2Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the California State Parks help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I'm a student. Do you have internships?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"The California State Parks hires Student Assistants \"\n",
    "                \"to perform a variety of tasks that require limited or no previous \"\n",
    "                \"work experience.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"Cool, how do I sign up?\"},\n",
    "        ],\n",
    "        \"generate_inputs\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "# Alternate conversation that can also be used with the code here:\n",
    "# chat_input = Granite3Point2Inputs.model_validate(\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             {\n",
    "#                 \"role\": \"assistant\",\n",
    "#                 \"content\": \"Welcome to the Alameda County Tourism help desk.\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": \"I'm in downtown Dublin, and I like to visit old houses. \"\n",
    "#                 \"Is there a good one to visit?\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"role\": \"assistant\",\n",
    "#                 \"content\": \"You might want to visit the Kolb House.\",\n",
    "#             },\n",
    "#             {\"role\": \"user\", \"content\": \"Where is it?\"},\n",
    "#         ],\n",
    "#         \"generate_inputs\": {\n",
    "#             \"temperature\": 0.0,\n",
    "#             \"max_tokens\": 4096,\n",
    "#         },\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by passing the chat completion request directly to the language model,\n",
    "without using retrieval-augmented generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an IO processor for the base model\n",
    "io_proc = make_io_processor(model_name, backend=backend)\n",
    "\n",
    "# Use the IO processor to generate a chat completion\n",
    "non_rag_result = io_proc.create_chat_completion(chat_input)\n",
    "display(Markdown(non_rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Insert description of hallucinations in the response*\n",
    "\n",
    "*TODO: Insert link to web page with correct information*\n",
    "\n",
    "Now let's spin up an in-memory vector database, using embeddings that we've precomputed\n",
    "offline from the MTRAG benchmark's government corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an in-memory vector database\n",
    "retriever = InMemoryRetriever(embeddings_data_file, embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vector database fetches document snippets that match a given query.\n",
    "# For example, the user's question in the conversation above:\n",
    "print(f\"Query is: '{chat_input.messages[-1].content}'\")\n",
    "print(\"Matching document snippets:\")\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "retriever.retrieve(chat_input.messages[-1].content, top_k=3).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attach a RequestProcessor to our vector database so that we can augment the chat \n",
    "completion request with retrieved document snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_request_proc = RetrievalRequestProcessor(retriever, top_k=3)\n",
    "chat_input_with_docs = retrieval_request_proc.process(chat_input)[0]\n",
    "{\n",
    "    k: v\n",
    "    for k, v in chat_input_with_docs.model_dump().items()\n",
    "    if k in (\"messages\", \"documents\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retriever operates over the last user turn.\n",
    "\n",
    "*TODO: Describe critical information that is not in the last turn*\n",
    "\n",
    "The snippets retrieved are not specific to the user's intended question.\n",
    "\n",
    "Let's see what happens if we run our request through the model using the low-quality \n",
    "RAG snippets from the previous cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_result = io_proc.create_chat_completion(chat_input_with_docs)\n",
    "display(Markdown(rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Describe what is wrong about this response*\n",
    "\n",
    "We can use the [LoRA Adapter for Answerability Classification](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-answerability-prediction)\n",
    "to detect this kind of problem. Here's what happens if we run the chat completion \n",
    "request with faulty documents snippets through the answerability model, using the\n",
    "`granite_io` IO processor for the model to handle input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerability_proc = AnswerabilityIOProcessor(answerability_lora_backend)\n",
    "answerability_proc.create_chat_completion(chat_input_with_docs).results[\n",
    "    0\n",
    "].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answerability model detects that the documents we have retrieved cannot be used to\n",
    "answer the user's question. We use use a composite IO processor to wrap this check in\n",
    "a flow that falls back on canned response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerability_composite_proc = AnswerabilityCompositeIOProcessor(\n",
    "    io_proc, answerability_proc\n",
    ")\n",
    "composite_result = answerability_composite_proc.create_chat_completion(\n",
    "    chat_input_with_docs\n",
    ").results[0]\n",
    "print(composite_result.next_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use use the [LoRA Adapter for Query Rewrite](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-query-rewrite) to rewrite\n",
    "the last user turn into a string that is more useful for retrieiving document snippets.\n",
    "Here's what we get if we call this model directly on the original request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_io_proc = QueryRewriteIOProcessor(query_rewrite_lora_backend)\n",
    "rewrite_io_proc.create_chat_completion(chat_input).results[0].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap the IO processor for this model in a request processor that rewrites\n",
    "the last turn of the chat completion request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_request_proc = RewriteRequestProcessor(rewrite_io_proc)\n",
    "rewritten_chat_input = rewrite_request_proc.process(chat_input)[0]\n",
    "print(\"Messages after rewrite:\")\n",
    "[{\"role\": m.role, \"content\": m.content} for m in rewritten_chat_input.messages]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Temporarily disabled pending a bug fix this Friday\n",
    "rewritten_chat_input_with_docs = retrieval_request_proc.process(rewritten_chat_input)[0]\n",
    "answerability_proc.create_chat_completion(rewritten_chat_input_with_docs).results[\n",
    "    0\n",
    "].next_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewritten_chat_input_with_docs = retrieval_request_proc.process(rewritten_chat_input)[0]\n",
    "chat_input_with_docs_from_rewrite = rewritten_chat_input_with_docs.model_copy(\n",
    "    update={\"messages\": chat_input.messages}\n",
    ")\n",
    "answerability_proc.create_chat_completion(chat_input_with_docs_from_rewrite).results[\n",
    "    0\n",
    "].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain all of these request processors together with the IO processor for \n",
    "the answerability model to create a single flow that processes requests in multiple\n",
    "steps:\n",
    "1. Rewrite the last user message for retrieval\n",
    "1. Retrieve documents and attach them to the request\n",
    "1. Check for answerability with the retrieved documents\n",
    "1. If the answerability check passes, then send the request to the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_request_proc = RewriteRequestProcessor(rewrite_io_proc)\n",
    "\n",
    "request = rewrite_request_proc.process(chat_input)[0]\n",
    "request = retrieval_request_proc.process(request)[0]\n",
    "\n",
    "# Switch back to original version of last turn\n",
    "request = request.model_copy(update={\"messages\": chat_input.messages})\n",
    "\n",
    "# Check for answerability and generate if documents pass the check\n",
    "response = answerability_composite_proc.create_chat_completion(request)\n",
    "rag_rewrite_result = response.results[0]\n",
    "display(Markdown(rag_rewrite_result.next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Insert description of how this response is better.*\n",
    "\n",
    "We can use the [LoRA Adapter for Citation Generation](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-citation-generation\n",
    ") to explain exactly how this response is grounded in the documents that the rewritten\n",
    "user query retrieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for answerability, generate a response, then add citations\n",
    "citations_composite_proc = CitationsCompositeIOProcessor(\n",
    "    answerability_composite_proc, citations_lora_backend\n",
    ")\n",
    "result_with_citations = citations_composite_proc.create_chat_completion(\n",
    "    request\n",
    ").results[0]\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(result_with_citations.next_message.content))\n",
    "print(\"Citations:\")\n",
    "pd.set_option(\"display.max_colwidth\", 1500)\n",
    "pd.DataFrame.from_records(\n",
    "    [c.model_dump() for c in result_with_citations.next_message.citations]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full set of documents for reference\n",
    "for doc_index, doc in enumerate(request.documents):\n",
    "    print(f\"Document {doc_index}:\")\n",
    "    display(Markdown(doc.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw output of the citations model is available for debugging\n",
    "result_with_citations.next_message.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a hallucination check over the preceding response\n",
    "hallucinations_composite_proc = HallucinationsCompositeIOProcessor(\n",
    "    io_proc, hallucination_lora_backend\n",
    ")\n",
    "result_with_hallucinations = hallucinations_composite_proc.create_chat_completion(\n",
    "    request\n",
    ").results[0]\n",
    "result_with_hallucinations.next_message.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap all of the functionality we've shown so far in a single class that \n",
    "inherits from the `InputOutputProcessor` interface in `granite-io`. Packaging things\n",
    "this way lets applications treat this multi-step flow as if it was a single chat \n",
    "completion request to a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.base import InputOutputProcessor, RequestProcessor\n",
    "from granite_io.backend import Backend\n",
    "from granite_io.io.base import ChatCompletionInputs, ChatCompletionResults\n",
    "\n",
    "\n",
    "class GraniteRAGCompositeIOProcessor(InputOutputProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        io_proc: InputOutputProcessor,\n",
    "        rewrite_request_proc: RequestProcessor,\n",
    "        retrieval_request_proc: RequestProcessor,\n",
    "        answerability_proc: InputOutputProcessor,\n",
    "        citations_lora_backend: Backend,\n",
    "        hallucination_lora_backend: Backend,\n",
    "    ):\n",
    "        self.rewrite_request_proc = rewrite_request_proc\n",
    "        self.retrieval_request_proc = retrieval_request_proc\n",
    "\n",
    "        # Build up a chain of IO processors:\n",
    "        # answerability -> Granite -> citations -> hallucinations\n",
    "        chain = AnswerabilityCompositeIOProcessor(io_proc, answerability_proc)\n",
    "        chain = CitationsCompositeIOProcessor(chain, citations_lora_backend)\n",
    "        chain = HallucinationsCompositeIOProcessor(chain, hallucination_lora_backend)\n",
    "        self.io_proc_chain = chain\n",
    "\n",
    "    async def acreate_chat_completion(\n",
    "        self, inputs: ChatCompletionInputs\n",
    "    ) -> ChatCompletionResults:\n",
    "        \"\"\"\n",
    "        Chat completions API inherited from the ``InputOutputProcessor`` base class.\n",
    "\n",
    "        :param inputs: Structured representation of the inputs to a chat completion\n",
    "            request, possibly including additional fields that only this input-output\n",
    "            processor can consume\n",
    "\n",
    "        :returns: The next message that the model produces when fed the specified\n",
    "            inputs, plus additional information about the low-level request.\n",
    "        \"\"\"\n",
    "        original_inputs = inputs\n",
    "\n",
    "        # Rewrite and retrieve\n",
    "        inputs = (await rewrite_request_proc.aprocess(inputs))[0]\n",
    "        inputs = (await retrieval_request_proc.aprocess(inputs))[0]\n",
    "\n",
    "        # Switch back to original version of last turn\n",
    "        inputs = inputs.model_copy(update={\"messages\": original_inputs.messages})\n",
    "\n",
    "        # Perform answerability check, generate a response, add citations, and check\n",
    "        # for hallucinations.\n",
    "        return await self.io_proc_chain.acreate_chat_completion(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_io_proc = GraniteRAGCompositeIOProcessor(\n",
    "    io_proc,\n",
    "    rewrite_request_proc=rewrite_request_proc,\n",
    "    retrieval_request_proc=retrieval_request_proc,\n",
    "    answerability_proc=answerability_proc,\n",
    "    citations_lora_backend=citations_lora_backend,\n",
    "    hallucination_lora_backend=hallucination_lora_backend,\n",
    ")\n",
    "\n",
    "rag_result = rag_io_proc.create_chat_completion(chat_input).results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assistant response:\")\n",
    "display(Markdown(rag_result.next_message.content))\n",
    "print(\"Citations:\")\n",
    "pd.set_option(\"display.max_colwidth\", 1500)\n",
    "display(\n",
    "    pd.DataFrame.from_records(\n",
    "        [c.model_dump() for c in rag_result.next_message.citations]\n",
    "    )\n",
    ")\n",
    "print(\"Hallucination Checks:\")\n",
    "display(\n",
    "    pd.DataFrame.from_records(\n",
    "        [h.model_dump() for h in rag_result.next_message.hallucinations]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
