{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5816dd-e93b-4b30-a40c-aa67eea3d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from granite_io.io.granite_3_3.input_processors.granite_3_3_input_processor import (\n",
    "    Granite3Point3Inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cab3feb-3144-402d-9b16-7a665a344835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.hyde_rewrite import HyDERewriteIOProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e587cf-2e9f-4ee8-a1a9-28b41bdb1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ibm-granite/granite-3.3-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a2f40f-2978-4261-afb1-c397663bbb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01:51:21 Running: /dccstor/vineeku6-seq2seq/code/granite-io/.venv/bin/vllm serve ibm-granite/granite-3.3-8b-instruct --port 48007 --gpu-memory-utilization 0.45 --max-model-len 32768 --guided_decoding_backend outlines --device auto --enforce-eager\n"
     ]
    }
   ],
   "source": [
    "server = LocalVLLMServer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f0bd83-8139-4de4-b98a-f171c62ec964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 01:52:10 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-07 01:52:23 [api_server.py:1043] vLLM API server version 0.8.5.post1\n",
      "INFO 06-07 01:52:23 [api_server.py:1044] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.3-8b-instruct', config='', host=None, port=48007, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.3-8b-instruct', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', max_model_len=32768, guided_decoding_backend='outlines', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.45, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x14c78afb7be0>)\n",
      "INFO 06-07 01:52:42 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'embed', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 06-07 01:52:42 [arg_utils.py:1658] --guided-decoding-backend=outlines is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 06-07 01:52:42 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 06-07 01:52:42 [api_server.py:246] Started engine process with PID 2969073\n",
      "INFO 06-07 01:52:53 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-07 01:52:58 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='ibm-granite/granite-3.3-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.3-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=ibm-granite/granite-3.3-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \n",
      "INFO 06-07 01:53:00 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 06-07 01:53:04 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-07 01:53:04 [model_runner.py:1108] Starting to load model ibm-granite/granite-3.3-8b-instruct...\n",
      "INFO 06-07 01:53:05 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:07<00:23,  7.93s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:15<00:15,  7.77s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:17<00:05,  5.27s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:25<00:00,  6.13s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:25<00:00,  6.34s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 01:53:30 [loader.py:458] Loading weights took 25.47 seconds\n",
      "INFO 06-07 01:53:31 [model_runner.py:1140] Model loading took 15.2512 GiB and 26.748777 seconds\n",
      "INFO 06-07 01:53:34 [worker.py:287] Memory profiling takes 3.16 seconds\n",
      "INFO 06-07 01:53:34 [worker.py:287] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.45) = 35.66GiB\n",
      "INFO 06-07 01:53:34 [worker.py:287] model weights take 15.25GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 3.35GiB; the rest of the memory reserved for KV Cache is 16.97GiB.\n",
      "INFO 06-07 01:53:34 [executor_base.py:112] # cuda blocks: 6950, # CPU blocks: 1638\n",
      "INFO 06-07 01:53:34 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 3.39x\n",
      "INFO 06-07 01:53:38 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 6.95 seconds\n",
      "INFO 06-07 01:53:38 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:48007\n",
      "INFO 06-07 01:53:38 [launcher.py:28] Available routes are:\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /docs, Methods: GET, HEAD\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /health, Methods: GET\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /load, Methods: GET\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /ping, Methods: GET, POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /tokenize, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /detokenize, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /v1/models, Methods: GET\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /version, Methods: GET\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /pooling, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /score, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /v1/score, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /rerank, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /invocations, Methods: POST\n",
      "INFO 06-07 01:53:38 [launcher.py:36] Route: /metrics, Methods: GET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [2968748]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:57620 - \"GET /ping HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "server.wait_for_startup(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb56a9b3-eeed-4603-9b2e-aa772fb28d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = server.make_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "414a8a63-0a9f-48c2-a281-a8eb8df63bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Granite3Point3Inputs(messages=[AssistantMessage(content='Welcome to pet questions!', role='assistant', tool_calls=[], reasoning_content=None, citations=None, documents=None, hallucinations=None, stop_reason=None), UserMessage(content='I have two pets, a dog named Rex and a cat named Lucy.', role='user'), AssistantMessage(content='Great, what would you like to share about them?', role='assistant', tool_calls=[], reasoning_content=None, citations=None, documents=None, hallucinations=None, stop_reason=None), UserMessage(content='Rex spends a lot of time in the backyard and outdoors, and Luna is always inside.', role='user'), AssistantMessage(content='Sounds good! Rex must love exploring outside, while Lucy probably enjoys her cozy indoor life.', role='assistant', tool_calls=[], reasoning_content=None, citations=None, documents=None, hallucinations=None, stop_reason=None), UserMessage(content='But is he more likely to get fleas because of that?', role='user')], tools=[], generate_inputs=GenerateInputs(prompt=None, model=None, best_of=None, echo=None, frequency_penalty=None, logit_bias=None, logprobs=None, max_tokens=None, n=None, presence_penalty=None, stop=None, stream=None, stream_options=None, suffix=None, temperature=0.0, top_p=None, user=None, extra_headers=None, extra_body={}), documents=[], controls=None, thinking=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an example chat completion with a short conversation.\n",
    "chat_input = Granite3Point3Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"assistant\", \"content\": \"Welcome to pet questions!\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I have two pets, a dog named Rex and a cat named Lucy.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Great, what would you like to share about them?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Rex spends a lot of time in the backyard and outdoors, \"\n",
    "                \"and Luna is always inside.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Sounds good! Rex must love exploring outside, while Lucy \"\n",
    "                \"probably enjoys her cozy indoor life.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"But is he more likely to get fleas because of that?\",\n",
    "            },\n",
    "        ],\n",
    "        \"generate_inputs\": {\"temperature\": 0.0},\n",
    "    }\n",
    ")\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002ea7c5-1fc5-4c21-8bcb-639cc6f74095",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_proc = HyDERewriteIOProcessor(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c93827-f7c8-4520-a146-eb9aa647b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 01:53:42 [logger.py:39] Received request cmpl-edba0feabe88457eb6055fdf051ea0e9-0: prompt: \"<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday's Date: June 07, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>Generate a short answer for the user query below. Do not use more than 50 words.\\nBut is he more likely to get fleas because of that?<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 2946, 49153, 39558, 390, 17071, 2821, 44, 30468, 225, 36, 34, 36, 38, 32, 203, 23669, 1182, 2821, 44, 30422, 225, 34, 41, 30, 225, 36, 34, 36, 39, 32, 203, 4282, 884, 8080, 278, 659, 30, 18909, 810, 25697, 32, 2448, 884, 312, 17247, 19551, 47330, 32, 0, 203, 49152, 496, 49153, 10505, 312, 4504, 7592, 436, 322, 1256, 2467, 4794, 32, 3278, 646, 793, 1829, 2784, 225, 39, 34, 8153, 32, 203, 11275, 438, 938, 1829, 11353, 372, 622, 296, 274, 302, 3301, 432, 688, 49, 0, 203, 49152, 17594, 49153], lora_request: None, prompt_adapter_request: None.\n",
      "INFO 06-07 01:53:42 [engine.py:310] Added request cmpl-edba0feabe88457eb6055fdf051ea0e9-0.\n",
      "INFO 06-07 01:53:43 [metrics.py:486] Avg prompt throughput: 18.8 tokens/s, Avg generation throughput: 3.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO:     127.0.0.1:57634 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "{\n",
      "  \"content\": \"But is he more likely to get fleas because of that? Yes, pets with longer fur or those frequently in contact with infested areas are more prone to flea infestations. Regular grooming and preventive treatments can help minimize this risk.\",\n",
      "  \"role\": \"user\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chat_result = await io_proc.acreate_chat_completion(chat_input)\n",
    "print(chat_result.results[0].next_message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eed2eca-1a7f-480f-a0d0-b304b539958e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResults(results=[ChatCompletionResult(next_message=UserMessage(content='But is he more likely to get fleas because of that? Yes, pets with longer fur or those frequently in contact with infested areas are more prone to flea infestations. Regular grooming and preventive treatments can help minimize this risk.', role='user'))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 01:53:53 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 01:54:03 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "chat_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e44729-e7fe-45f3-b6c8-c41c82bde29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a471a2d-7c86-4038-80a4-3144e5711e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
