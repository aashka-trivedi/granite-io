{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end demo with MTRAG benchmark data\n",
    "\n",
    "This notebook shows several examples of end-to-end RAG use cases that use the retrieval\n",
    "IO processor in conjunction with the IO processors for other Granite-based LoRA \n",
    "adapters.\n",
    "\n",
    "This notebook can run its own vLLM server to perform inference, or you can host the \n",
    "models on your own server. To use your own server, set the `run_server` variable below\n",
    "to `False` and set appropriate values for the constants in the cell marked\n",
    "`# Constants go here`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from granite_io.io.granite_3_2.input_processors.granite_3_2_input_processor import (\n",
    "    Granite3Point2Inputs,\n",
    ")\n",
    "from granite_io import make_io_processor, make_backend\n",
    "from granite_io.io.base import RewriteRequestProcessor\n",
    "from granite_io.io.retrieval.util import download_mtrag_embeddings\n",
    "from granite_io.io.retrieval import InMemoryRetriever, RetrievalRequestProcessor\n",
    "from granite_io.io.answerability import (\n",
    "    AnswerabilityIOProcessor,\n",
    "    AnswerabilityCompositeIOProcessor,\n",
    ")\n",
    "from granite_io.io.query_rewrite import QueryRewriteIOProcessor\n",
    "from granite_io.io.citations import CitationsCompositeIOProcessor\n",
    "from granite_io.backend.vllm_server import LocalVLLMServer\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "temp_data_dir = \"../data/test_retrieval_temp\"\n",
    "corpus_name = \"govt\"\n",
    "embeddings_data_file = pathlib.Path(temp_data_dir) / f\"{corpus_name}_embeds.parquet\"\n",
    "embedding_model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "model_name = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "query_rewrite_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-query-rewrite\"\n",
    "citations_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-citation-generation\"\n",
    "answerability_lora_name = \"ibm-granite/granite-3.2-8b-lora-rag-answerability-prediction\"\n",
    "\n",
    "run_server = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_server:\n",
    "    # Start by firing up a local vLLM server and connecting a backend instance to it.\n",
    "    server = LocalVLLMServer(\n",
    "        model_name,\n",
    "        lora_adapters=[\n",
    "            (lora_name, lora_name)\n",
    "            for lora_name in (\n",
    "                query_rewrite_lora_name,\n",
    "                citations_lora_name,\n",
    "                answerability_lora_name,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    server.wait_for_startup(200)\n",
    "    query_rewrite_lora_backend = server.make_lora_backend(query_rewrite_lora_name)\n",
    "    citations_lora_backend = server.make_lora_backend(citations_lora_name)\n",
    "    answerability_lora_backend = server.make_lora_backend(answerability_lora_name)\n",
    "    backend = server.make_backend()\n",
    "else:  # if not run_server\n",
    "    # Use an existing server.\n",
    "    # The constants here are for the server that local_vllm_server.ipynb starts.\n",
    "    # Modify as needed.\n",
    "    openai_base_url = \"http://localhost:55555/v1\"\n",
    "    openai_api_key = \"granite_intrinsics_1234\"\n",
    "    backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    query_rewrite_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": query_rewrite_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    citations_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": citations_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )\n",
    "    answerability_lora_backend = make_backend(\n",
    "        \"openai\",\n",
    "        {\n",
    "            \"model_name\": answerability_lora_name,\n",
    "            \"openai_base_url\": openai_base_url,\n",
    "            \"openai_api_key\": openai_api_key,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the indexed corpus if it hasn't already been downloaded.\n",
    "# This notebook uses a subset of the government corpus from the MTRAG benchmark.\n",
    "embeddings_location = f\"{temp_data_dir}/{corpus_name}_embeds.parquet\"\n",
    "if not os.path.exists(embeddings_location):\n",
    "    download_mtrag_embeddings(embedding_model_name, corpus_name, embeddings_location)\n",
    "embeddings_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an IO processor for the base model\n",
    "io_proc = make_io_processor(model_name, backend=backend)\n",
    "io_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating an example chat completion request. The user is chatting with an\n",
    "agent for the California Appellate Courts help desk and is currently asking for \n",
    "information about the opening hours of the law library at the Solano County Hall of\n",
    "Justice in Fairfield, CA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completions request\n",
    "chat_input = Granite3Point2Inputs.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the California Appellate Courts help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I have a court appearance next Tuesday at the Solano \"\n",
    "                \"County Hall of Justice. Can I use the law library to prepare?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Yes, you can visit the law library. Law libraries are a \"\n",
    "                \"great resource for self-representation or learning about the law.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"What's its address?\"},\n",
    "        ],\n",
    "        \"generate_inputs\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by passing the chat completion request directly to the language model,\n",
    "without using retrieval-augmented generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_rag_result = io_proc.create_chat_completion(chat_input)\n",
    "display(Markdown(non_rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's output contains a hallucinated address. \n",
    "See [this website](https://solanolibrary.com/hours-and-locations/law-library/) for \n",
    "the actual location of the Solano County courthouse's law library.\n",
    "\n",
    "Now let's spin up an in-memory vector database, using embeddings that we've precomputed\n",
    "offline from the MTRAG benchmark's government corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an in-memory vector database\n",
    "retriever = InMemoryRetriever(embeddings_data_file, embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attach a RequestProcessor to our vector database so that we can augment the chat \n",
    "completion request with retrieved document snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_request_proc = RetrievalRequestProcessor(retriever, top_k=3)\n",
    "rag_chat_input = retrieval_request_proc.process(chat_input)[0]\n",
    "rag_chat_input.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retriever operates over the last user turn, and this turn does not mention \n",
    "Fairfield or Solano County, so the snippets retrieved will not be specific to the \n",
    "Solano County courthouse.\n",
    "\n",
    "Let's see what happens if we run our request through the model using the low-quality \n",
    "RAG snippets from the previous cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_result = io_proc.create_chat_completion(rag_chat_input)\n",
    "display(Markdown(rag_result.results[0].next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This address is also incorrect due to the low-quality retrieved snippets.\n",
    "\n",
    "We can use the [LoRA Adapter for Answerability Classification](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-answerability-prediction)\n",
    "to detect this kind of problem. Here's what happens if we run the chat completion \n",
    "request with faulty documents snippets through the answerability model, using the\n",
    "`granite_io` IO processor for the model to handle input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerability_proc = AnswerabilityIOProcessor(answerability_lora_backend)\n",
    "answerability_proc.create_chat_completion(rag_chat_input).results[\n",
    "    0\n",
    "].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answerability model detects that the documents we have retrieved cannot be used to\n",
    "answer the user's question. We use use a composite IO processor to wrap this check in\n",
    "a flow that falls back on canned response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerability_composite_proc = AnswerabilityCompositeIOProcessor(\n",
    "    io_proc, answerability_proc\n",
    ")\n",
    "composite_result = answerability_composite_proc.create_chat_completion(\n",
    "    rag_chat_input\n",
    ").results[0]\n",
    "composite_result.next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use use the [LoRA Adapter for Query Rewrite](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-query-rewrite) to rewrite\n",
    "the last user turn into a string that is more useful for retrieiving document snippets.\n",
    "Here's what we get if we call this model directly on the original request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_io_proc = QueryRewriteIOProcessor(query_rewrite_lora_backend)\n",
    "rewrite_io_proc.create_chat_completion(chat_input).results[0].next_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap the IO processor for this model in a request processor that rewrites\n",
    "the last turn of the chat completion request, then chain that request processor to\n",
    "the request processor for retrieval that we used earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_request_proc = RewriteRequestProcessor(rewrite_io_proc)\n",
    "\n",
    "request = rewrite_request_proc.process(chat_input)[0]\n",
    "request = retrieval_request_proc.process(request)[0]\n",
    "rag_rewrite_result = io_proc.create_chat_completion(request).results[0]\n",
    "display(Markdown(rag_rewrite_result.next_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we get the correct address!\n",
    "\n",
    "We can also augment the above result with citations back to the supporting documents\n",
    "by using the [LoRA Adapter for Citation Generation](\n",
    "    https://huggingface.co/ibm-granite/granite-3.2-8b-lora-rag-citation-generation\n",
    ")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_composite_proc = CitationsCompositeIOProcessor(\n",
    "    io_proc, citations_lora_backend\n",
    ")\n",
    "result_with_citations = citations_composite_proc.create_chat_completion(\n",
    "    request\n",
    ").results[0]\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(result_with_citations.next_message.content))\n",
    "print(\"Citations:\")\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "pd.DataFrame.from_records(\n",
    "    [c.model_dump() for c in result_with_citations.next_message.citations]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
